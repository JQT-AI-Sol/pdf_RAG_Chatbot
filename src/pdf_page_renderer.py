"""
PDF Page Rendering Module

PDFã®ç‰¹å®šãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ›ã—ã¦Streamlit UIã§è¡¨ç¤ºã™ã‚‹æ©Ÿèƒ½ã‚’æä¾›
"""

import logging
import tempfile
from pathlib import Path
from typing import Optional, Tuple, List, Dict
import streamlit as st
from PIL import Image, ImageDraw
import pdfplumber

logger = logging.getLogger(__name__)

# pdf2imageã®å‹•ä½œç¢ºèªï¼ˆpopplerãŒå¿…è¦ï¼‰
PDF2IMAGE_AVAILABLE = False
try:
    from pdf2image import convert_from_path
    PDF2IMAGE_AVAILABLE = True
    logger.info("=" * 60)
    logger.info("âœ… PDF2IMAGE_AVAILABLE = True")
    logger.info("âœ… pdf2image is available - PDF page rendering ENABLED")
    logger.info("âœ… poppler-utils found - Highlights will work")
    logger.info("=" * 60)
except Exception as e:
    logger.warning("=" * 60)
    logger.warning("âŒ PDF2IMAGE_AVAILABLE = False")
    logger.warning(f"âŒ pdf2image not available: {e}")
    logger.warning("âŒ PDF page preview will be DISABLED")
    logger.warning("ðŸ’¡ Check packages.txt contains: poppler-utils")
    logger.warning("=" * 60)

# ç”»åƒç”Ÿæˆã®è¨­å®š
DEFAULT_DPI = 150  # æ¨™æº–å“è³ª
DEFAULT_WIDTH = 1000  # ãƒ”ã‚¯ã‚»ãƒ«å¹…


def get_pdf_path(source_file: str, vector_store) -> Optional[Path]:
    """
    PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—ï¼ˆå¿…è¦ã«å¿œã˜ã¦Supabase Storageã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰

    Args:
        source_file: PDFãƒ•ã‚¡ã‚¤ãƒ«å
        vector_store: VectorStoreã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆSupabase Storageé€£æºç”¨ï¼‰

    Returns:
        Path: PDFã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã€å–å¾—å¤±æ•—æ™‚ã¯None
    """
    # ã¾ãšãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’ãƒã‚§ãƒƒã‚¯
    local_pdf_path = Path("data/uploaded_pdfs") / source_file

    if local_pdf_path.exists():
        logger.info(f"Using local PDF: {local_pdf_path}")
        return local_pdf_path

    # Supabase Storageã‹ã‚‰ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    if vector_store and vector_store.provider == 'supabase':
        try:
            temp_dir = Path(tempfile.gettempdir()) / "rag_pdf_cache"
            temp_dir.mkdir(exist_ok=True)
            temp_pdf_path = temp_dir / source_file

            # æ—¢ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if temp_pdf_path.exists():
                logger.info(f"Using cached PDF: {temp_pdf_path}")
                return temp_pdf_path

            # Supabase Storageã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            success = vector_store.download_pdf_from_storage(source_file, str(temp_pdf_path))

            if success and temp_pdf_path.exists():
                logger.info(f"Downloaded PDF from Supabase Storage: {temp_pdf_path}")
                return temp_pdf_path
            else:
                logger.error(f"Failed to download PDF from Supabase Storage: {source_file}")
                return None

        except Exception as e:
            logger.error(f"Error accessing PDF from Supabase Storage: {e}")
            return None

    logger.error(f"PDF not found: {source_file}")
    return None


@st.cache_data(ttl=3600, show_spinner=False)
def extract_page_as_image(
    source_file: str,
    page_number: int,
    _vector_store,  # Streamlitã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ã«ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ä»˜ã
    dpi: int = DEFAULT_DPI,
    target_width: int = DEFAULT_WIDTH
) -> Optional[Image.Image]:
    """
    PDFã®ç‰¹å®šãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ›

    Args:
        source_file: PDFãƒ•ã‚¡ã‚¤ãƒ«å
        page_number: ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆ1å§‹ã¾ã‚Šï¼‰
        _vector_store: VectorStoreã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‹ã‚‰é™¤å¤–ï¼‰
        dpi: è§£åƒåº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 150ï¼‰
        target_width: ç”»åƒå¹…ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000ï¼‰

    Returns:
        PIL.Image: å¤‰æ›ã•ã‚ŒãŸç”»åƒã€å¤±æ•—æ™‚ã¯None
    """
    if not PDF2IMAGE_AVAILABLE:
        logger.warning("PDF page rendering is disabled (poppler not installed)")
        return None

    try:
        # PDFã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
        pdf_path = get_pdf_path(source_file, _vector_store)
        if not pdf_path:
            logger.error(f"Failed to get PDF path: {source_file}")
            return None

        # PDFãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ›ï¼ˆæŒ‡å®šãƒšãƒ¼ã‚¸ã®ã¿ï¼‰
        # page_numberã¯1å§‹ã¾ã‚Šã ãŒã€first_pageã¨last_pageã‚‚1å§‹ã¾ã‚Šã§æŒ‡å®š
        logger.info(f"Converting page {page_number} of {source_file} to image (DPI: {dpi})")

        images = convert_from_path(
            str(pdf_path),
            dpi=dpi,
            first_page=page_number,
            last_page=page_number,
            fmt='png'
        )

        if not images or len(images) == 0:
            logger.error(f"No image generated for page {page_number} of {source_file}")
            return None

        image = images[0]

        # ç”»åƒã‚µã‚¤ã‚ºã‚’èª¿æ•´ï¼ˆæ¨ªå¹…ã‚’æŒ‡å®šå¹…ã«åˆã‚ã›ã¦ç¸¦æ¨ªæ¯”ç¶­æŒï¼‰
        if image.width > target_width:
            aspect_ratio = image.height / image.width
            new_height = int(target_width * aspect_ratio)
            image = image.resize((target_width, new_height), Image.Resampling.LANCZOS)
            logger.info(f"Resized image to {target_width}x{new_height}")

        logger.info(f"Successfully converted page {page_number} of {source_file}")
        return image

    except Exception as e:
        logger.error(f"Error converting PDF page to image: {e}", exc_info=True)
        return None


@st.cache_data(ttl=3600, show_spinner=False)
def extract_multiple_pages(
    source_file: str,
    page_numbers: list[int],
    _vector_store,
    dpi: int = DEFAULT_DPI,
    target_width: int = DEFAULT_WIDTH
) -> dict[int, Optional[Image.Image]]:
    """
    è¤‡æ•°ãƒšãƒ¼ã‚¸ã‚’ä¸€æ‹¬ã§ç”»åƒã«å¤‰æ›ï¼ˆåŠ¹çŽ‡åŒ–ï¼‰

    Args:
        source_file: PDFãƒ•ã‚¡ã‚¤ãƒ«å
        page_numbers: ãƒšãƒ¼ã‚¸ç•ªå·ã®ãƒªã‚¹ãƒˆï¼ˆ1å§‹ã¾ã‚Šï¼‰
        _vector_store: VectorStoreã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        dpi: è§£åƒåº¦
        target_width: ç”»åƒå¹…

    Returns:
        dict: {page_number: Image.Image}ã®è¾žæ›¸
    """
    if not PDF2IMAGE_AVAILABLE:
        logger.warning("PDF page rendering is disabled (poppler not installed)")
        return {page: None for page in page_numbers}

    results = {}

    if not page_numbers:
        return results

    try:
        # PDFã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
        pdf_path = get_pdf_path(source_file, _vector_store)
        if not pdf_path:
            logger.error(f"Failed to get PDF path: {source_file}")
            return {page: None for page in page_numbers}

        # ãƒšãƒ¼ã‚¸ç•ªå·ã§ã‚½ãƒ¼ãƒˆï¼ˆåŠ¹çŽ‡çš„ãªæŠ½å‡ºã®ãŸã‚ï¼‰
        sorted_pages = sorted(page_numbers)

        logger.info(f"Converting {len(sorted_pages)} pages of {source_file} to images")

        # å„ãƒšãƒ¼ã‚¸ã‚’å€‹åˆ¥ã«å¤‰æ›ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡ã®ãŸã‚ï¼‰
        for page_num in sorted_pages:
            try:
                images = convert_from_path(
                    str(pdf_path),
                    dpi=dpi,
                    first_page=page_num,
                    last_page=page_num,
                    fmt='png'
                )

                if images and len(images) > 0:
                    image = images[0]

                    # ãƒªã‚µã‚¤ã‚º
                    if image.width > target_width:
                        aspect_ratio = image.height / image.width
                        new_height = int(target_width * aspect_ratio)
                        image = image.resize((target_width, new_height), Image.Resampling.LANCZOS)

                    results[page_num] = image
                    logger.debug(f"Converted page {page_num}")
                else:
                    results[page_num] = None
                    logger.warning(f"No image for page {page_num}")

            except Exception as e:
                logger.error(f"Error converting page {page_num}: {e}")
                results[page_num] = None

        logger.info(f"Successfully converted {len([v for v in results.values() if v is not None])}/{len(sorted_pages)} pages")
        return results

    except Exception as e:
        logger.error(f"Error in batch page conversion: {e}", exc_info=True)
        return {page: None for page in page_numbers}


def tokenize_query(query: str) -> List[str]:
    """
    æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆæ—¥æœ¬èªžå½¢æ…‹ç´ è§£æžï¼‰

    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª

    Returns:
        list: ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ
    """
    if not query or not query.strip():
        logger.warning("Empty query provided for tokenization")
        return []

    try:
        import MeCab
        mecab = MeCab.Tagger("-Owakati")
        tokens = mecab.parse(query).strip().split()
        logger.info(f"âœ… MeCab tokenization successful: '{query}' -> {tokens}")
        return tokens
    except ImportError as e:
        logger.warning(f"âš ï¸ MeCab not available ({e}), using Japanese-aware fallback")
        # MeCabãŒä½¿ãˆãªã„å ´åˆã¯æ—¥æœ¬èªžå¯¾å¿œãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        tokens = _japanese_aware_tokenize(query)
        logger.info(f"Fallback tokenization: '{query}' -> {tokens}")
        return tokens
    except Exception as e:
        logger.error(f"âŒ Error tokenizing query: {e}")
        tokens = _japanese_aware_tokenize(query)
        logger.info(f"Error fallback tokenization: '{query}' -> {tokens}")
        return tokens


def _japanese_aware_tokenize(text: str) -> List[str]:
    """
    æ—¥æœ¬èªžå¯¾å¿œã®ç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆMeCabãŒä½¿ãˆãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

    Args:
        text: ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        list: ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ
    """
    import re

    # è¨˜å·ãƒ»å¥èª­ç‚¹ã‚’å‰Šé™¤
    text = re.sub(r'[ã€Œã€ã€Žã€ã€ã€‘ã€ã€‚ï¼Ÿï¼ãƒ»\s]+', ' ', text)

    # è‹±æ•°å­—ã¨æ—¥æœ¬èªžæ–‡å­—ã‚’åˆ†é›¢
    tokens = []
    current_token = ""

    for char in text:
        if char.isspace():
            if current_token:
                tokens.append(current_token)
                current_token = ""
        else:
            current_token += char
            # 2-3æ–‡å­—ã”ã¨ã«åŒºåˆ‡ã‚‹ï¼ˆæ—¥æœ¬èªžã®å ´åˆï¼‰
            if len(current_token) >= 3 and not char.isascii():
                tokens.append(current_token)
                current_token = ""

    if current_token:
        tokens.append(current_token)

    # çŸ­ã™ãŽã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å¤–ï¼ˆ1æ–‡å­—ã®ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãªã©ï¼‰
    tokens = [t for t in tokens if len(t) >= 2 or t.isalnum()]

    return tokens


def extract_keywords_llm(query: str, _rag_engine) -> List[str]:
    """
    LLMã‚’ä½¿ç”¨ã—ã¦ã‚¯ã‚¨ãƒªã‹ã‚‰é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã‚’æŠ½å‡º

    Args:
        query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª
        _rag_engine: RAGEngineã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆLLMã‚¢ã‚¯ã‚»ã‚¹ç”¨ï¼‰

    Returns:
        list: æŠ½å‡ºã•ã‚ŒãŸé‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
    """
    if not query or not query.strip():
        logger.warning("Empty query provided for LLM keyword extraction")
        return []

    if not _rag_engine:
        logger.warning("RAGEngine not available for LLM keyword extraction")
        return tokenize_query(query)

    try:
        from langchain_core.messages import HumanMessage

        prompt = f"""ä»¥ä¸‹ã®è³ªå•ã‹ã‚‰ã€PDFãƒšãƒ¼ã‚¸ä¸Šã§ãƒã‚¤ãƒ©ã‚¤ãƒˆã™ã¹ãé‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

**é™¤å¤–ã™ã¹ãã‚‚ã®:**
- åŠ©è©žï¼ˆã®ã€ã¯ã€ã‚’ã€ãŒã€ã«ã€ã§ã€ã¨ã€ã‚„ã€ã‹ã‚‰ã€ã¾ã§ã€ã‚ˆã‚Šã€ã¸ï¼‰
- æŒ‡ç¤ºèªžï¼ˆã“ã®ã€ãã®ã€ã‚ã®ã€ã©ã®ã€ã©ã‚Œã€ã„ã¤ã€ã©ã“ï¼‰
- ä¸€èˆ¬çš„ãªå‹•è©žï¼ˆã™ã‚‹ã€ã‚ã‚‹ã€ã„ã‚‹ã€ãªã‚‹ã€è¡Œã†ã€ç¤ºã™ï¼‰
- ç–‘å•è©žå˜ä½“ï¼ˆä½•ã€èª°ã€ã„ã¤ã€ã©ã“ã€ãªãœã€ã©ã†ï¼‰
- 1-2æ–‡å­—ã®æ–­ç‰‡ã‚„æ´»ç”¨èªžå°¾

**æŠ½å‡ºã™ã¹ãã‚‚ã®:**
- åè©žï¼ˆç‰¹ã«å›ºæœ‰åè©žã€å°‚é–€ç”¨èªžã€çµ„ç¹”åã€äººåï¼‰
- é‡è¦ãªå‹•è©žãƒ»å½¢å®¹è©žï¼ˆæ ¸å¿ƒçš„ãªå‹•ä½œã‚„çŠ¶æ…‹ï¼‰
- æ•°å€¤ã‚„æ—¥ä»˜
- è¤‡åˆèªžï¼ˆä¾‹: ã€Œå› æžœé–¢ä¿‚ã€ã€Œèªå®šå¦èªã€ï¼‰

è³ªå•: {query}

é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ãƒ³ãƒžåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜Žä¸è¦ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ï¼‰:"""

        # LLMå‘¼ã³å‡ºã—ï¼ˆtemperature=0ã§ç¢ºå®šçš„ãªå‡ºåŠ›ï¼‰
        response = _rag_engine.openai_llm.invoke([HumanMessage(content=prompt)])
        keywords_text = response.content.strip()

        # ã‚«ãƒ³ãƒžã¾ãŸã¯ã‚¹ãƒšãƒ¼ã‚¹ã§åˆ†å‰²
        keywords = []
        for k in keywords_text.replace('ã€', ',').split(','):
            k = k.strip()
            if k and len(k) >= 2:  # 1æ–‡å­—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯é™¤å¤–
                keywords.append(k)

        logger.info(f"ðŸ¤– LLM keyword extraction: '{query}' -> {keywords}")
        return keywords if keywords else tokenize_query(query)

    except Exception as e:
        logger.error(f"âŒ LLM keyword extraction failed: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—¢å­˜ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        fallback_keywords = tokenize_query(query)
        logger.info(f"Fallback to tokenization: {fallback_keywords}")
        return fallback_keywords


def find_text_positions(
    pdf_path: Path,
    page_number: int,
    search_terms: List[str],
    vision_analyzer=None,
    dpi: int = DEFAULT_DPI
) -> List[Dict[str, float]]:
    """
    PDFãƒšãƒ¼ã‚¸å†…ã§æŒ‡å®šã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®åº§æ¨™ã‚’æ¤œå‡º

    Args:
        pdf_path: PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        page_number: ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆ1å§‹ã¾ã‚Šï¼‰
        search_terms: æ¤œç´¢ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        vision_analyzer: VisionAnalyzerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆOCRãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã€çœç•¥å¯ï¼‰
        dpi: OCRç”¨ç”»åƒã®è§£åƒåº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 150ï¼‰

    Returns:
        list: åº§æ¨™æƒ…å ±ã®ãƒªã‚¹ãƒˆ
            [{
                "text": str,  # ãƒžãƒƒãƒã—ãŸãƒ†ã‚­ã‚¹ãƒˆ
                "x0": float,  # å·¦ç«¯Xåº§æ¨™
                "y0": float,  # ä¸Šç«¯Yåº§æ¨™
                "x1": float,  # å³ç«¯Xåº§æ¨™
                "y1": float,  # ä¸‹ç«¯Yåº§æ¨™
            }]
    """
    positions = []

    if not search_terms:
        return positions

    try:
        with pdfplumber.open(pdf_path) as pdf:
            # ãƒšãƒ¼ã‚¸ç•ªå·ã¯1å§‹ã¾ã‚Šã ãŒã€pdfplumberã¯0å§‹ã¾ã‚Š
            page = pdf.pages[page_number - 1]

            # ãƒšãƒ¼ã‚¸å†…ã®å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªžå˜ä½ã§å–å¾—
            words = page.extract_words()

            # OCRãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: pdfplumberã§æŠ½å‡ºã§ããªã„å ´åˆ
            if len(words) == 0 and vision_analyzer and PDF2IMAGE_AVAILABLE:
                logger.warning(f"âš ï¸ PDF page {page_number} has no extractable text - attempting OCR")

                try:
                    # PDFãƒšãƒ¼ã‚¸ã‚’ç”»åƒåŒ–
                    images = convert_from_path(
                        str(pdf_path),
                        dpi=dpi,
                        first_page=page_number,
                        last_page=page_number,
                        fmt='png'
                    )

                    if images and len(images) > 0:
                        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
                        import tempfile
                        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp_file:
                            tmp_path = tmp_file.name
                            images[0].save(tmp_path, 'PNG')

                        try:
                            # Vision API OCRå®Ÿè¡Œ
                            logger.info(f"ðŸ” Running OCR on page {page_number} using Vision API...")
                            ocr_result = vision_analyzer.ocr_page(tmp_path)
                            words = ocr_result.get("words", [])
                            logger.info(f"âœ… OCR extracted {len(words)} words from page {page_number}")
                        finally:
                            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                            import os
                            if os.path.exists(tmp_path):
                                os.remove(tmp_path)
                    else:
                        logger.error(f"âŒ Failed to convert PDF page {page_number} to image for OCR")
                except Exception as ocr_error:
                    logger.error(f"âŒ OCR fallback failed: {ocr_error}", exc_info=True)
                    # OCRå¤±æ•—æ™‚ã¯ç©ºã®wordsã®ã¾ã¾ç¶šè¡Œ

            # å„æ¤œç´¢èªžã«å¯¾ã—ã¦ãƒžãƒƒãƒãƒ³ã‚°ã‚’å®Ÿè¡Œ
            for search_term in search_terms:
                search_term_lower = search_term.lower()

                for word in words:
                    word_text = word['text'].lower()

                    # éƒ¨åˆ†ä¸€è‡´ã§ãƒžãƒƒãƒãƒ³ã‚°
                    if search_term_lower in word_text or word_text in search_term_lower:
                        positions.append({
                            "text": word['text'],
                            "x0": word['x0'],
                            "y0": word['top'],
                            "x1": word['x1'],
                            "y1": word['bottom'],
                        })

            logger.info(f"ðŸ“ Found {len(positions)} text positions for {len(search_terms)} search terms on page {page_number}")
            if len(positions) == 0 and len(search_terms) > 0:
                logger.warning(f"âš ï¸ No text positions found for search terms: {search_terms}")
            return positions

    except Exception as e:
        logger.error(f"âŒ Error finding text positions: {e}", exc_info=True)
        return []


def highlight_text_on_image(
    image: Image.Image,
    text_positions: List[Dict[str, float]],
    page_height: float,
    dpi: int = DEFAULT_DPI,
    highlight_color: Tuple[int, int, int, int] = (255, 255, 0, 80)  # é»„è‰²åŠé€æ˜Ž
) -> Image.Image:
    """
    ç”»åƒä¸Šã«ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’æç”»

    Args:
        image: å…ƒã®ç”»åƒ
        text_positions: ãƒ†ã‚­ã‚¹ãƒˆåº§æ¨™ã®ãƒªã‚¹ãƒˆ
        page_height: PDFãƒšãƒ¼ã‚¸ã®é«˜ã•ï¼ˆãƒã‚¤ãƒ³ãƒˆå˜ä½ï¼‰
        dpi: ç”»åƒã®DPI
        highlight_color: ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚«ãƒ©ãƒ¼ (R, G, B, Alpha)

    Returns:
        PIL.Image: ãƒã‚¤ãƒ©ã‚¤ãƒˆä»˜ãç”»åƒ
    """
    if not text_positions:
        return image

    try:
        # RGBAãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›ï¼ˆé€éŽå‡¦ç†ã®ãŸã‚ï¼‰
        if image.mode != 'RGBA':
            image = image.convert('RGBA')

        # é€æ˜Žãªã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã‚’ä½œæˆ
        overlay = Image.new('RGBA', image.size, (255, 255, 255, 0))
        draw = ImageDraw.Draw(overlay)

        # PDFåº§æ¨™ã‹ã‚‰ç”»åƒåº§æ¨™ã¸ã®ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°
        # PDFã¯72 DPIã€ç”»åƒã¯DPIæŒ‡å®šã®è§£åƒåº¦
        scale = dpi / 72.0

        # ãƒšãƒ¼ã‚¸é«˜ã•ã®ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´
        img_height = image.height

        # å„ãƒ†ã‚­ã‚¹ãƒˆä½ç½®ã«ãƒã‚¤ãƒ©ã‚¤ãƒˆçŸ©å½¢ã‚’æç”»
        for pos in text_positions:
            # PDFåº§æ¨™ï¼ˆåŽŸç‚¹ã¯å·¦ä¸‹ï¼‰ã‹ã‚‰ç”»åƒåº§æ¨™ï¼ˆåŽŸç‚¹ã¯å·¦ä¸Šï¼‰ã«å¤‰æ›
            x0 = pos['x0'] * scale
            y0 = (page_height - pos['y1']) * scale  # Yåº§æ¨™ã¯åè»¢
            x1 = pos['x1'] * scale
            y1 = (page_height - pos['y0']) * scale

            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¿½åŠ ï¼ˆèª­ã¿ã‚„ã™ãã™ã‚‹ãŸã‚ï¼‰
            padding = 2
            draw.rectangle(
                [(x0 - padding, y0 - padding), (x1 + padding, y1 + padding)],
                fill=highlight_color
            )

        # ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã‚’å…ƒç”»åƒã«åˆæˆ
        highlighted = Image.alpha_composite(image, overlay)

        # RGBãƒ¢ãƒ¼ãƒ‰ã«æˆ»ã™ï¼ˆStreamlitã§ã®è¡¨ç¤ºã®ãŸã‚ï¼‰
        highlighted = highlighted.convert('RGB')

        logger.info(f"Applied {len(text_positions)} highlights to image")
        return highlighted

    except Exception as e:
        logger.error(f"Error highlighting text on image: {e}", exc_info=True)
        return image


@st.cache_data(ttl=3600, show_spinner=False, max_entries=100)
def extract_page_with_highlight(
    source_file: str,
    page_number: int,
    query: str,
    _vector_store,
    _rag_engine=None,
    _vision_analyzer=None,
    use_llm_keywords: bool = True,
    cache_version: int = 4,  # v4: OCR fallback implementation (NO underscore to include in cache key!)
    dpi: int = DEFAULT_DPI,
    target_width: int = DEFAULT_WIDTH
) -> Optional[Image.Image]:
    """
    PDFãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ›ã—ã€æ¤œç´¢ã‚¯ã‚¨ãƒªã«ä¸€è‡´ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ

    Args:
        source_file: PDFãƒ•ã‚¡ã‚¤ãƒ«å
        page_number: ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆ1å§‹ã¾ã‚Šï¼‰
        query: æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆãƒã‚¤ãƒ©ã‚¤ãƒˆå¯¾è±¡ï¼‰
        _vector_store: VectorStoreã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        _rag_engine: RAGEngineã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆLLMã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã«ä½¿ç”¨ã€çœç•¥å¯ï¼‰
        _vision_analyzer: VisionAnalyzerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆOCRãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã€çœç•¥å¯ï¼‰
        use_llm_keywords: LLMã‚’ä½¿ç”¨ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚’æœ‰åŠ¹åŒ–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
        cache_version: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆå¤‰æ›´æ™‚ã«ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆã€é€šå¸¸å¤‰æ›´ä¸è¦ï¼‰
        dpi: è§£åƒåº¦
        target_width: ç”»åƒå¹…

    Returns:
        PIL.Image: ãƒã‚¤ãƒ©ã‚¤ãƒˆä»˜ãç”»åƒã€å¤±æ•—æ™‚ã¯None
    """
    # ðŸ” å®Ÿè¡Œç¢ºèªãƒ­ã‚°ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    logger.info(f"ðŸ“¸ extract_page_with_highlight() CALLED - cache_v{cache_version}, pdf2image={PDF2IMAGE_AVAILABLE}")
    logger.info(f"   â†’ source={source_file}, page={page_number}, query_len={len(query) if query else 0}")

    if not PDF2IMAGE_AVAILABLE:
        logger.warning("=" * 60)
        logger.warning("âŒ PDF page rendering is DISABLED (poppler not installed)")
        logger.warning(f"âŒ Cannot render page {page_number} of {source_file}")
        logger.warning("ðŸ’¡ Check Streamlit Cloud logs for poppler installation errors")
        logger.warning("=" * 60)
        return None

    try:
        # PDFã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
        pdf_path = get_pdf_path(source_file, _vector_store)
        if not pdf_path:
            logger.error(f"Failed to get PDF path: {source_file}")
            return None

        # ã‚¯ã‚¨ãƒªã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆLLM or ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼‰
        logger.info(f"ðŸ” Highlighting query: '{query}' for {source_file} page {page_number}")

        if query:
            # LLMãƒ™ãƒ¼ã‚¹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆæŽ¨å¥¨ï¼‰
            if use_llm_keywords and _rag_engine is not None:
                try:
                    search_terms = extract_keywords_llm(query, _rag_engine)
                    logger.info(f"ðŸ¤– LLM-extracted keywords: {search_terms}")
                except Exception as e:
                    logger.warning(f"âš ï¸ LLM keyword extraction failed: {e}, falling back to tokenization")
                    search_terms = tokenize_query(query)
                    logger.info(f"ðŸ”¤ Fallback tokenized keywords: {search_terms}")
            else:
                # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                search_terms = tokenize_query(query)
                logger.info(f"ðŸ”¤ Tokenized keywords: {search_terms}")
        else:
            search_terms = []

        # ãƒ†ã‚­ã‚¹ãƒˆä½ç½®ã‚’æ¤œå‡º
        text_positions = []
        page_height = 0

        if search_terms:
            with pdfplumber.open(pdf_path) as pdf:
                page = pdf.pages[page_number - 1]
                page_height = page.height
                text_positions = find_text_positions(pdf_path, page_number, search_terms, _vision_analyzer, dpi)
                logger.info(f"ðŸ“Š Text positions found: {len(text_positions)}")
        else:
            logger.warning("âš ï¸ No search terms to highlight")

        # ãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ›
        images = convert_from_path(
            str(pdf_path),
            dpi=dpi,
            first_page=page_number,
            last_page=page_number,
            fmt='png'
        )

        if not images or len(images) == 0:
            logger.error(f"No image generated for page {page_number} of {source_file}")
            return None

        image = images[0]

        # ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’é©ç”¨
        if text_positions and page_height > 0:
            image = highlight_text_on_image(image, text_positions, page_height, dpi)
            logger.info(f"âœ… Applied {len(text_positions)} highlights to page {page_number} of {source_file}")
        elif search_terms:
            logger.warning(f"âš ï¸ No highlights applied (text_positions={len(text_positions)}, page_height={page_height})")

        # ç”»åƒã‚µã‚¤ã‚ºã‚’èª¿æ•´
        if image.width > target_width:
            aspect_ratio = image.height / image.width
            new_height = int(target_width * aspect_ratio)
            image = image.resize((target_width, new_height), Image.Resampling.LANCZOS)

        return image

    except Exception as e:
        logger.error(f"Error extracting page with highlight: {e}", exc_info=True)
        return None
