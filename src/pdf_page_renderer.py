"""
PDF Page Rendering Module

PDFã®ç‰¹å®šãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ›ã—ã¦Streamlit UIã§è¡¨ç¤ºã™ã‚‹æ©Ÿèƒ½ã‚’æä¾›
"""

import logging
import tempfile
from pathlib import Path
from typing import Optional, Tuple, List, Dict
import streamlit as st
from PIL import Image, ImageDraw
import pdfplumber

logger = logging.getLogger(__name__)

# PyMuPDF (fitz) ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ - ãƒã‚¤ãƒ©ã‚¤ãƒˆåº§æ¨™ã®é«˜é€Ÿæ¤œç´¢ç”¨
PYMUPDF_AVAILABLE = False
try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
    logger.info("âœ… PyMuPDF is available - Fast PDF text search enabled")
except ImportError:
    logger.warning("âŒ PyMuPDF not available - Using pdfplumber fallback")

# pdf2imageã®å‹•ä½œç¢ºèªï¼ˆpopplerãŒå¿…è¦ï¼‰
PDF2IMAGE_AVAILABLE = False
try:
    from pdf2image import convert_from_path
    PDF2IMAGE_AVAILABLE = True
    logger.info("=" * 60)
    logger.info("âœ… PDF2IMAGE_AVAILABLE = True")
    logger.info("âœ… pdf2image is available - PDF page rendering ENABLED")
    logger.info("âœ… poppler-utils found - Highlights will work")
    logger.info("=" * 60)
except Exception as e:
    logger.warning("=" * 60)
    logger.warning("âŒ PDF2IMAGE_AVAILABLE = False")
    logger.warning(f"âŒ pdf2image not available: {e}")
    logger.warning("âŒ PDF page preview will be DISABLED")
    logger.warning("ğŸ’¡ Check packages.txt contains: poppler-utils")
    logger.warning("=" * 60)

# ç”»åƒç”Ÿæˆã®è¨­å®š
DEFAULT_DPI = 150  # æ¨™æº–å“è³ª
DEFAULT_WIDTH = 1000  # ãƒ”ã‚¯ã‚»ãƒ«å¹…


def get_pdf_path(source_file: str, vector_store) -> Optional[Path]:
    """
    PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—ï¼ˆå¿…è¦ã«å¿œã˜ã¦Supabase Storageã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰
    Officeâ†’PDFå¤‰æ›ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚å¯¾å¿œ

    Args:
        source_file: PDFãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã¾ãŸã¯Officeãƒ•ã‚¡ã‚¤ãƒ«åï¼‰
        vector_store: VectorStoreã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆSupabase Storageé€£æºç”¨ï¼‰

    Returns:
        Path: PDFã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã€å–å¾—å¤±æ•—æ™‚ã¯None
    """
    # ã¾ãšãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’ãƒã‚§ãƒƒã‚¯
    local_pdf_path = Path("data/uploaded_pdfs") / source_file

    if local_pdf_path.exists():
        logger.info(f"Using local PDF: {local_pdf_path}")
        return local_pdf_path

    # Officeâ†’PDFå¤‰æ›æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
    # source_fileãŒ.docx, .xlsx, .pptxç­‰ã®å ´åˆã€.pdfã«å¤‰æ›ã—ã¦æ¤œç´¢
    source_path = Path(source_file)
    office_extensions = ['.docx', '.doc', '.xlsx', '.xls', '.pptx', '.ppt']

    if source_path.suffix.lower() in office_extensions:
        # æ‹¡å¼µå­ã‚’.pdfã«å¤‰æ›´
        pdf_filename = source_path.stem + ".pdf"

        # å¤‰æ›æ¸ˆã¿PDFãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒã‚§ãƒƒã‚¯
        converted_pdf_path = Path("data/converted_pdfs") / pdf_filename
        if converted_pdf_path.exists():
            logger.info(f"Using converted PDF: {converted_pdf_path}")
            return converted_pdf_path

        # static/pdfsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚ãƒã‚§ãƒƒã‚¯
        static_pdf_path = Path("static/pdfs") / pdf_filename
        if static_pdf_path.exists():
            logger.info(f"Using static converted PDF: {static_pdf_path}")
            return static_pdf_path

    # Supabase Storageã‹ã‚‰ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    if vector_store and vector_store.provider == 'supabase':
        try:
            temp_dir = Path(tempfile.gettempdir()) / "rag_pdf_cache"
            temp_dir.mkdir(exist_ok=True)
            temp_pdf_path = temp_dir / source_file

            # æ—¢ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if temp_pdf_path.exists():
                logger.info(f"Using cached PDF: {temp_pdf_path}")
                return temp_pdf_path

            # Supabase Storageã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            success = vector_store.download_pdf_from_storage(source_file, str(temp_pdf_path))

            if success and temp_pdf_path.exists():
                logger.info(f"Downloaded PDF from Supabase Storage: {temp_pdf_path}")
                return temp_pdf_path
            else:
                logger.error(f"Failed to download PDF from Supabase Storage: {source_file}")
                return None

        except Exception as e:
            logger.error(f"Error accessing PDF from Supabase Storage: {e}")
            return None

    logger.error(f"PDF not found: {source_file}")
    return None


def create_pdf_annotations_pymupdf(
    pdf_path: Path,
    search_terms: List[str],
    page_numbers: Optional[List[int]] = None
) -> List[Dict]:
    """
    PyMuPDFã‚’ä½¿ç”¨ã—ã¦PDFå†…ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢ã—ã€streamlit-pdf-viewerç”¨ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ

    Args:
        pdf_path: PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        search_terms: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
        page_numbers: æ¤œç´¢å¯¾è±¡ã®ãƒšãƒ¼ã‚¸ç•ªå·ãƒªã‚¹ãƒˆï¼ˆ1å§‹ã¾ã‚Šï¼‰ã€‚Noneã®å ´åˆã¯å…¨ãƒšãƒ¼ã‚¸

    Returns:
        List[Dict]: streamlit-pdf-viewerç”¨ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å½¢å¼
            [
                {
                    "page": 1,
                    "x": 220,
                    "y": 155,
                    "width": 65,
                    "height": 22,
                    "color": "yellow",
                    "border": "solid"
                },
                ...
            ]
    """
    import unicodedata

    logger.info(f"ğŸ” create_pdf_annotations_pymupdf() called")
    logger.info(f"   pdf_path={pdf_path}")
    logger.info(f"   search_terms={search_terms}")
    logger.info(f"   page_numbers={page_numbers}")
    logger.info(f"   PYMUPDF_AVAILABLE={PYMUPDF_AVAILABLE}")

    if not PYMUPDF_AVAILABLE:
        logger.warning("PyMuPDF not available - cannot create annotations")
        return []

    annotations = []

    try:
        doc = fitz.open(pdf_path)
        logger.info(f"âœ… PDF opened successfully: {len(doc)} pages")

        # æ¤œç´¢å¯¾è±¡ãƒšãƒ¼ã‚¸ã®æ±ºå®š
        if page_numbers is None:
            page_numbers = list(range(1, len(doc) + 1))

        for page_num in page_numbers:
            try:
                # ãƒšãƒ¼ã‚¸ç•ªå·ã¯1å§‹ã¾ã‚Šã ãŒã€PyMuPDFã¯0å§‹ã¾ã‚Š
                page = doc[page_num - 1]
                page_height = page.rect.height

                # ãƒšãƒ¼ã‚¸ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆæ¤œè¨¼ç”¨ï¼‰
                page_text = page.get_text()
                logger.info(f"ğŸ“„ Processing page {page_num}: size={page.rect}, height={page_height}")
                logger.info(f"   Page text length: {len(page_text)} chars")
                if len(page_text) > 0:
                    logger.debug(f"   First 100 chars: {page_text[:100]}")
                else:
                    logger.warning(f"   âš ï¸ Page {page_num} has NO extractable text (might be scanned)")

                for term in search_terms:
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é•·ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ2æ–‡å­—ä»¥ä¸Šã®ã¿ï¼‰
                    if len(term) < 2:
                        logger.debug(f"   Skipping term '{term}' (too short)")
                        continue

                    logger.info(f"   ğŸ” Searching for: '{term}' (len={len(term)})")

                    # Unicodeæ­£è¦åŒ–ï¼ˆNFCå½¢å¼ï¼‰
                    term_normalized = unicodedata.normalize('NFC', term)
                    if term_normalized != term:
                        logger.info(f"      Unicode normalized: '{term}' â†’ '{term_normalized}'")

                    # ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ï¼ˆçŸ©å½¢ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼‰
                    rects = page.search_for(term_normalized)
                    logger.info(f"      â†’ Found {len(rects)} matches for '{term_normalized}' on page {page_num}")

                    # NFCæ­£è¦åŒ–ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€NFDå½¢å¼ã‚‚è©¦ã™
                    if len(rects) == 0:
                        term_nfd = unicodedata.normalize('NFD', term)
                        if term_nfd != term_normalized:
                            logger.info(f"      Trying NFD normalization: '{term_nfd}'")
                            rects = page.search_for(term_nfd)
                            logger.info(f"      â†’ NFD search found {len(rects)} matches")

                    # ãã‚Œã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ãƒšãƒ¼ã‚¸ãƒ†ã‚­ã‚¹ãƒˆå†…ã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                    if len(rects) == 0 and len(page_text) > 0:
                        if term in page_text or term_normalized in page_text:
                            logger.warning(f"      âš ï¸ Term '{term}' exists in page text but search_for() returned 0 results!")
                            logger.warning(f"         This might be an encoding issue")
                        else:
                            logger.debug(f"      â„¹ï¸ Term '{term}' not found in page text")

                    for rect in rects:
                        # PyMuPDFåº§æ¨™ï¼ˆå·¦ä¸‹åŸç‚¹ï¼‰â†’ streamlit-pdf-vieweråº§æ¨™ï¼ˆå·¦ä¸ŠåŸç‚¹ï¼‰
                        annotations.append({
                            "page": page_num,
                            "x": float(rect.x0),
                            "y": float(page_height - rect.y1),  # Yåº§æ¨™ã‚’åè»¢
                            "width": float(rect.x1 - rect.x0),
                            "height": float(rect.y1 - rect.y0),
                            "color": "yellow",
                            "border": "solid"
                        })

                page_annotations = len([a for a in annotations if a['page'] == page_num])
                logger.info(f"   ğŸ“ Created {page_annotations} annotations for page {page_num}")

            except Exception as e:
                logger.warning(f"Error processing page {page_num}: {e}", exc_info=True)
                continue

        doc.close()
        logger.info(f"ğŸ“Š Summary: Created {len(annotations)} annotations for {len(search_terms)} search terms")
        if len(annotations) == 0:
            logger.warning(f"   âš ï¸ NO ANNOTATIONS CREATED despite {len(search_terms)} search terms!")
        return annotations

    except Exception as e:
        logger.error(f"Error creating annotations: {e}", exc_info=True)
        return []


def split_text_into_sentences(text: str) -> List[Dict]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡å˜ä½ã«åˆ†å‰²ã—ã€å„æ–‡ã®é–‹å§‹ãƒ»çµ‚äº†ä½ç½®ã‚’è¨˜éŒ²

    Args:
        text: åˆ†å‰²å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        List[Dict]: æ–‡ã®ãƒªã‚¹ãƒˆ
            [
                {
                    "text": "æ–‡ã®å†…å®¹",
                    "start": 0,  # æ–‡å­—ã‚ªãƒ•ã‚»ãƒƒãƒˆï¼ˆé–‹å§‹ä½ç½®ï¼‰
                    "end": 10    # æ–‡å­—ã‚ªãƒ•ã‚»ãƒƒãƒˆï¼ˆçµ‚äº†ä½ç½®ï¼‰
                },
                ...
            ]
    """
    import re

    if not text or not text.strip():
        return []

    sentences = []

    # æ—¥æœ¬èªãƒ»è‹±èªå¯¾å¿œã®æ–‡åŒºåˆ‡ã‚Šãƒ‘ã‚¿ãƒ¼ãƒ³
    # ã€‚ï¼.!ï¼?ï¼Ÿã§åŒºåˆ‡ã‚‹
    pattern = r'([^ã€‚ï¼.!ï¼?ï¼Ÿ]+[ã€‚\.!ï¼?ï¼Ÿ]+)'

    matches = re.finditer(pattern, text)

    for match in matches:
        sentence_text = match.group(0).strip()
        if len(sentence_text) > 0:
            sentences.append({
                "text": sentence_text,
                "start": match.start(),
                "end": match.end()
            })

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã—ãªã„æ®‹ã‚Šã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæœ€å¾Œã®æ–‡ãªã©ï¼‰
    if sentences:
        last_end = sentences[-1]["end"]
        if last_end < len(text):
            remaining = text[last_end:].strip()
            if len(remaining) > 0:
                sentences.append({
                    "text": remaining,
                    "start": last_end,
                    "end": len(text)
                })
    elif len(text.strip()) > 0:
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã—ãªã„å ´åˆã€å…¨ä½“ã‚’1æ–‡ã¨ã—ã¦æ‰±ã†
        sentences.append({
            "text": text.strip(),
            "start": 0,
            "end": len(text)
        })

    logger.debug(f"Split text into {len(sentences)} sentences")
    return sentences


def filter_sentences_by_embedding(
    sentences: List[Dict],
    query: str,
    rag_engine,
    threshold: float = 0.7,
    max_candidates: int = 10
) -> List[Dict]:
    """
    ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã®é¡ä¼¼åº¦ã§æ–‡ã‚’çµã‚Šè¾¼ã¿

    Args:
        sentences: å€™è£œæ–‡ã®ãƒªã‚¹ãƒˆï¼ˆsplit_text_into_sentences()ã®å‡ºåŠ›ï¼‰
        query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª
        rag_engine: RAGEngineã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°è¨ˆç®—ç”¨ï¼‰
        threshold: é¡ä¼¼åº¦é–¾å€¤ï¼ˆ0-1ï¼‰
        max_candidates: æœ€å¤§å€™è£œæ•°

    Returns:
        List[Dict]: é¡ä¼¼åº¦ã®é«˜ã„æ–‡ã®ãƒªã‚¹ãƒˆï¼ˆé¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼‰
    """
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity

    if not sentences:
        return []

    try:
        # ã‚¯ã‚¨ãƒªã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’å–å¾—
        query_embedding = rag_engine.embedding_model.embed_query(query)

        # å„æ–‡ã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¨ˆç®—
        sentence_embeddings = []
        for sent in sentences:
            sent_embedding = rag_engine.embedding_model.embed_query(sent["text"])
            sentence_embeddings.append(sent_embedding)

        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
        similarities = cosine_similarity(
            [query_embedding],
            sentence_embeddings
        )[0]

        # å„æ–‡ã«é¡ä¼¼åº¦ã‚’è¿½åŠ 
        for i, sent in enumerate(sentences):
            sent["similarity"] = float(similarities[i])

        # é¡ä¼¼åº¦ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨ã‚½ãƒ¼ãƒˆ
        filtered = [s for s in sentences if s["similarity"] >= threshold]
        filtered.sort(key=lambda x: x["similarity"], reverse=True)

        # ä¸Šä½max_candidatesä»¶ã‚’è¿”ã™
        result = filtered[:max_candidates]

        logger.info(f"ğŸ” Embedding filter: {len(sentences)} sentences â†’ {len(result)} candidates (threshold={threshold})")
        for i, sent in enumerate(result[:3]):  # ä¸Šä½3ä»¶ã‚’ãƒ­ã‚°å‡ºåŠ›
            logger.debug(f"   {i+1}. similarity={sent['similarity']:.3f}: {sent['text'][:50]}...")

        return result

    except Exception as e:
        logger.error(f"Error in embedding filter: {e}", exc_info=True)
        return sentences[:max_candidates]  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ˆé ­ã‹ã‚‰è¿”ã™


def refine_with_llm(
    candidate_sentences: List[Dict],
    query: str,
    rag_engine,
    max_sentences: int = 5
) -> List[Dict]:
    """
    LLMã§å€™è£œæ–‡ã‚’ç²¾æŸ»ã—ã€æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„æ–‡ã‚’é¸æŠ

    Args:
        candidate_sentences: å€™è£œæ–‡ã®ãƒªã‚¹ãƒˆï¼ˆfilter_sentences_by_embedding()ã®å‡ºåŠ›ï¼‰
        query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª
        rag_engine: RAGEngineã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆLLMå‘¼ã³å‡ºã—ç”¨ï¼‰
        max_sentences: æœ€çµ‚é¸æŠã™ã‚‹æœ€å¤§æ–‡æ•°

    Returns:
        List[Dict]: LLMãŒé¸æŠã—ãŸé–¢é€£æ–‡ã®ãƒªã‚¹ãƒˆ
    """
    if not candidate_sentences:
        return []

    try:
        # å€™è£œæ–‡ã«ç•ªå·ã‚’ä»˜ã‘ã‚‹
        numbered_candidates = []
        for i, sent in enumerate(candidate_sentences):
            numbered_candidates.append(f"{i+1}. {sent['text']}")

        candidates_text = "\n".join(numbered_candidates)

        # LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = f"""ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒªã«æœ€ã‚‚é–¢é€£ã™ã‚‹æ–‡ã‚’ã€å€™è£œã‹ã‚‰æœ€å¤§{max_sentences}å€‹é¸ã‚“ã§ãã ã•ã„ã€‚

ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒªã€‘
{query}

ã€å€™è£œæ–‡ã€‘
{candidates_text}

ã€æŒ‡ç¤ºã€‘
- ä¸Šè¨˜ã®å€™è£œã‹ã‚‰ã€ã‚¯ã‚¨ãƒªã«ç›´æ¥é–¢é€£ã™ã‚‹æ–‡ã®ç•ªå·ã®ã¿ã‚’é¸ã‚“ã§ãã ã•ã„
- ç•ªå·ã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹: 1,3,5ï¼‰
- é–¢é€£ã™ã‚‹æ–‡ãŒãªã„å ´åˆã¯ã€Œãªã—ã€ã¨å‡ºåŠ›ã—ã¦ãã ã•ã„
- ç•ªå·ä»¥å¤–ã®èª¬æ˜ã¯ä¸è¦ã§ã™

ã€å‡ºåŠ›ã€‘
"""

        # LLMã‚’å‘¼ã³å‡ºã—
        from openai import OpenAI
        import os

        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = client.chat.completions.create(
            model="gpt-4.1",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            max_tokens=100
        )

        llm_response = response.choices[0].message.content.strip()
        logger.info(f"ğŸ¤– LLM refinement response: '{llm_response}'")

        # å¿œç­”ã‹ã‚‰ç•ªå·ã‚’æŠ½å‡º
        if "ãªã—" in llm_response or "None" in llm_response:
            logger.info(f"   LLM found no relevant sentences")
            return []

        # ç•ªå·ã‚’è§£æï¼ˆä¾‹: "1,3,5" â†’ [1, 3, 5]ï¼‰
        import re
        numbers = re.findall(r'\d+', llm_response)
        selected_indices = [int(n) - 1 for n in numbers if 0 <= int(n) - 1 < len(candidate_sentences)]

        selected_sentences = [candidate_sentences[i] for i in selected_indices]

        logger.info(f"   Selected {len(selected_sentences)} sentences from {len(candidate_sentences)} candidates")
        for sent in selected_sentences:
            logger.debug(f"      - {sent['text'][:50]}...")

        return selected_sentences

    except Exception as e:
        logger.error(f"Error in LLM refinement: {e}", exc_info=True)
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯é¡ä¼¼åº¦ä¸Šä½ã‚’è¿”ã™
        return candidate_sentences[:max_sentences]


def create_pdf_annotations_hybrid(
    pdf_path: Path,
    query: str,
    page_numbers: List[int],
    rag_engine,
    config: dict
) -> List[Dict]:
    """
    ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§PDFã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ

    Stage 1: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã§å€™è£œæ–‡ã‚’çµã‚Šè¾¼ã¿ï¼ˆé«˜é€Ÿï¼‰
    Stage 2: LLMã§é–¢é€£æ–‡ã‚’ç²¾æŸ»ï¼ˆé«˜ç²¾åº¦ï¼‰
    Stage 3: åº§æ¨™ã‚’å–å¾—ã—ã¦ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ

    Args:
        pdf_path: PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª
        page_numbers: æ¤œç´¢å¯¾è±¡ãƒšãƒ¼ã‚¸ç•ªå·ãƒªã‚¹ãƒˆï¼ˆ1å§‹ã¾ã‚Šï¼‰
        rag_engine: RAGEngineã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        config: è¨­å®šè¾æ›¸

    Returns:
        List[Dict]: streamlit-pdf-viewerç”¨ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å½¢å¼
    """
    import pdfplumber

    logger.info(f"ğŸ¯ create_pdf_annotations_hybrid() called")
    logger.info(f"   pdf_path={pdf_path}")
    logger.info(f"   query={query}")
    logger.info(f"   page_numbers={page_numbers}")

    # è¨­å®šã‚’å–å¾—
    hybrid_config = config.get("pdf_highlighting", {}).get("hybrid", {})
    embedding_threshold = hybrid_config.get("embedding_threshold", 0.7)
    max_candidates = hybrid_config.get("max_candidates", 10)
    max_final = hybrid_config.get("max_final", 5)
    use_llm_refinement = hybrid_config.get("use_llm_refinement", True)
    fallback_to_keyword = hybrid_config.get("fallback_to_keyword", True)

    annotations = []

    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num in page_numbers:
                try:
                    page = pdf.pages[page_num - 1]
                    page_text = page.extract_text()

                    if not page_text:
                        logger.warning(f"   Page {page_num} has no extractable text")
                        continue

                    logger.info(f"ğŸ“„ Processing page {page_num} ({len(page_text)} chars)")

                    # Stage 1: æ–‡åˆ†å‰²
                    sentences = split_text_into_sentences(page_text)
                    logger.info(f"   Stage 1: Split into {len(sentences)} sentences")

                    if not sentences:
                        continue

                    # Stage 2: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    candidates = filter_sentences_by_embedding(
                        sentences,
                        query,
                        rag_engine,
                        threshold=embedding_threshold,
                        max_candidates=max_candidates
                    )

                    if not candidates:
                        logger.info(f"   Stage 2: No candidates above threshold={embedding_threshold}")
                        continue

                    # Stage 3: LLMã§ç²¾æŸ»ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                    if use_llm_refinement and len(candidates) > 0:
                        selected_sentences = refine_with_llm(
                            candidates,
                            query,
                            rag_engine,
                            max_sentences=max_final
                        )
                    else:
                        selected_sentences = candidates[:max_final]

                    if not selected_sentences:
                        logger.info(f"   Stage 3: No sentences selected by LLM")
                        continue

                    # Stage 4: åº§æ¨™ã‚’å–å¾—ã—ã¦ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
                    page_height = page.height
                    for sent in selected_sentences:
                        # æ–‡ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åº§æ¨™ã‚’æ¤œç´¢
                        words = page.extract_words()
                        positions = find_text_positions_in_words(
                            sent["text"],
                            words,
                            page_num
                        )

                        # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã«å¤‰æ›
                        for pos in positions:
                            annotations.append({
                                "page": page_num,
                                "x": float(pos["x0"]),
                                "y": float(pos["y0"]),
                                "width": float(pos["x1"] - pos["x0"]),
                                "height": float(pos["y1"] - pos["y0"]),
                                "color": "yellow",
                                "border": "solid"
                            })

                    logger.info(f"   ğŸ“ Created {len(annotations)} annotations for page {page_num}")

                except Exception as e:
                    logger.error(f"Error processing page {page_num}: {e}", exc_info=True)
                    continue

        logger.info(f"ğŸ“Š Hybrid annotation summary: {len(annotations)} annotations created")
        return annotations

    except Exception as e:
        logger.error(f"Error in hybrid annotation generation: {e}", exc_info=True)

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ–¹å¼
        if fallback_to_keyword:
            logger.warning(f"   Falling back to keyword-based highlighting")
            from src.pdf_page_renderer import extract_keywords_llm
            keywords = extract_keywords_llm(query, rag_engine)
            return create_pdf_annotations_pymupdf(pdf_path, keywords, page_numbers)
        else:
            return []


def find_text_positions_in_words(
    search_text: str,
    words: List[Dict],
    page_number: int
) -> List[Dict]:
    """
    å˜èªãƒªã‚¹ãƒˆã‹ã‚‰æ¤œç´¢ãƒ†ã‚­ã‚¹ãƒˆã®åº§æ¨™ã‚’å–å¾—

    Args:
        search_text: æ¤œç´¢ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        words: pdfplumberã®extract_words()ã®å‡ºåŠ›
        page_number: ãƒšãƒ¼ã‚¸ç•ªå·

    Returns:
        List[Dict]: åº§æ¨™ã®ãƒªã‚¹ãƒˆ
    """
    positions = []
    search_text_lower = search_text.lower()

    # å˜èªã‚’ãƒ†ã‚­ã‚¹ãƒˆé †ã«çµåˆã—ã¦æ¤œç´¢
    for i, word in enumerate(words):
        word_text = word['text'].lower()

        # éƒ¨åˆ†ä¸€è‡´ã§æ¤œç´¢
        if search_text_lower in word_text or word_text in search_text_lower:
            positions.append({
                "text": word['text'],
                "x0": word['x0'],
                "y0": word['top'],
                "x1": word['x1'],
                "y1": word['bottom'],
            })

    return positions


@st.cache_data(ttl=3600, show_spinner=False)
def extract_page_as_image(
    source_file: str,
    page_number: int,
    _vector_store,  # Streamlitã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ã«ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ä»˜ã
    dpi: int = DEFAULT_DPI,
    target_width: int = DEFAULT_WIDTH
) -> Optional[Image.Image]:
    """
    PDFã®ç‰¹å®šãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ›

    Args:
        source_file: PDFãƒ•ã‚¡ã‚¤ãƒ«å
        page_number: ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆ1å§‹ã¾ã‚Šï¼‰
        _vector_store: VectorStoreã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‹ã‚‰é™¤å¤–ï¼‰
        dpi: è§£åƒåº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 150ï¼‰
        target_width: ç”»åƒå¹…ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000ï¼‰

    Returns:
        PIL.Image: å¤‰æ›ã•ã‚ŒãŸç”»åƒã€å¤±æ•—æ™‚ã¯None
    """
    if not PDF2IMAGE_AVAILABLE:
        logger.warning("PDF page rendering is disabled (poppler not installed)")
        return None

    try:
        # PDFã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
        pdf_path = get_pdf_path(source_file, _vector_store)
        if not pdf_path:
            logger.error(f"Failed to get PDF path: {source_file}")
            return None

        # PDFãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ›ï¼ˆæŒ‡å®šãƒšãƒ¼ã‚¸ã®ã¿ï¼‰
        # page_numberã¯1å§‹ã¾ã‚Šã ãŒã€first_pageã¨last_pageã‚‚1å§‹ã¾ã‚Šã§æŒ‡å®š
        logger.info(f"Converting page {page_number} of {source_file} to image (DPI: {dpi})")

        images = convert_from_path(
            str(pdf_path),
            dpi=dpi,
            first_page=page_number,
            last_page=page_number,
            fmt='png'
        )

        if not images or len(images) == 0:
            logger.error(f"No image generated for page {page_number} of {source_file}")
            return None

        image = images[0]

        # ç”»åƒã‚µã‚¤ã‚ºã‚’èª¿æ•´ï¼ˆæ¨ªå¹…ã‚’æŒ‡å®šå¹…ã«åˆã‚ã›ã¦ç¸¦æ¨ªæ¯”ç¶­æŒï¼‰
        if image.width > target_width:
            aspect_ratio = image.height / image.width
            new_height = int(target_width * aspect_ratio)
            image = image.resize((target_width, new_height), Image.Resampling.LANCZOS)
            logger.info(f"Resized image to {target_width}x{new_height}")

        logger.info(f"Successfully converted page {page_number} of {source_file}")
        return image

    except Exception as e:
        logger.error(f"Error converting PDF page to image: {e}", exc_info=True)
        return None


@st.cache_data(ttl=3600, show_spinner=False)
def extract_multiple_pages(
    source_file: str,
    page_numbers: list[int],
    _vector_store,
    dpi: int = DEFAULT_DPI,
    target_width: int = DEFAULT_WIDTH
) -> dict[int, Optional[Image.Image]]:
    """
    è¤‡æ•°ãƒšãƒ¼ã‚¸ã‚’ä¸€æ‹¬ã§ç”»åƒã«å¤‰æ›ï¼ˆåŠ¹ç‡åŒ–ï¼‰

    Args:
        source_file: PDFãƒ•ã‚¡ã‚¤ãƒ«å
        page_numbers: ãƒšãƒ¼ã‚¸ç•ªå·ã®ãƒªã‚¹ãƒˆï¼ˆ1å§‹ã¾ã‚Šï¼‰
        _vector_store: VectorStoreã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        dpi: è§£åƒåº¦
        target_width: ç”»åƒå¹…

    Returns:
        dict: {page_number: Image.Image}ã®è¾æ›¸
    """
    if not PDF2IMAGE_AVAILABLE:
        logger.warning("PDF page rendering is disabled (poppler not installed)")
        return {page: None for page in page_numbers}

    results = {}

    if not page_numbers:
        return results

    try:
        # PDFã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
        pdf_path = get_pdf_path(source_file, _vector_store)
        if not pdf_path:
            logger.error(f"Failed to get PDF path: {source_file}")
            return {page: None for page in page_numbers}

        # ãƒšãƒ¼ã‚¸ç•ªå·ã§ã‚½ãƒ¼ãƒˆï¼ˆåŠ¹ç‡çš„ãªæŠ½å‡ºã®ãŸã‚ï¼‰
        sorted_pages = sorted(page_numbers)

        logger.info(f"Converting {len(sorted_pages)} pages of {source_file} to images")

        # å„ãƒšãƒ¼ã‚¸ã‚’å€‹åˆ¥ã«å¤‰æ›ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®ãŸã‚ï¼‰
        for page_num in sorted_pages:
            try:
                images = convert_from_path(
                    str(pdf_path),
                    dpi=dpi,
                    first_page=page_num,
                    last_page=page_num,
                    fmt='png'
                )

                if images and len(images) > 0:
                    image = images[0]

                    # ãƒªã‚µã‚¤ã‚º
                    if image.width > target_width:
                        aspect_ratio = image.height / image.width
                        new_height = int(target_width * aspect_ratio)
                        image = image.resize((target_width, new_height), Image.Resampling.LANCZOS)

                    results[page_num] = image
                    logger.debug(f"Converted page {page_num}")
                else:
                    results[page_num] = None
                    logger.warning(f"No image for page {page_num}")

            except Exception as e:
                logger.error(f"Error converting page {page_num}: {e}")
                results[page_num] = None

        logger.info(f"Successfully converted {len([v for v in results.values() if v is not None])}/{len(sorted_pages)} pages")
        return results

    except Exception as e:
        logger.error(f"Error in batch page conversion: {e}", exc_info=True)
        return {page: None for page in page_numbers}


def tokenize_query(query: str) -> List[str]:
    """
    æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆæ—¥æœ¬èªå½¢æ…‹ç´ è§£æï¼‰

    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª

    Returns:
        list: ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ
    """
    if not query or not query.strip():
        logger.warning("Empty query provided for tokenization")
        return []

    try:
        import MeCab
        mecab = MeCab.Tagger("-Owakati")
        tokens = mecab.parse(query).strip().split()
        logger.info(f"âœ… MeCab tokenization successful: '{query}' -> {tokens}")
        return tokens
    except ImportError as e:
        logger.warning(f"âš ï¸ MeCab not available ({e}), using Japanese-aware fallback")
        # MeCabãŒä½¿ãˆãªã„å ´åˆã¯æ—¥æœ¬èªå¯¾å¿œãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        tokens = _japanese_aware_tokenize(query)
        logger.info(f"Fallback tokenization: '{query}' -> {tokens}")
        return tokens
    except Exception as e:
        logger.error(f"âŒ Error tokenizing query: {e}")
        tokens = _japanese_aware_tokenize(query)
        logger.info(f"Error fallback tokenization: '{query}' -> {tokens}")
        return tokens


def _japanese_aware_tokenize(text: str) -> List[str]:
    """
    æ—¥æœ¬èªå¯¾å¿œã®ç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆMeCabãŒä½¿ãˆãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

    Args:
        text: ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        list: ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ
    """
    import re

    # è¨˜å·ãƒ»å¥èª­ç‚¹ã‚’å‰Šé™¤
    text = re.sub(r'[ã€Œã€ã€ã€ã€ã€‘ã€ã€‚ï¼Ÿï¼ãƒ»\s]+', ' ', text)

    # è‹±æ•°å­—ã¨æ—¥æœ¬èªæ–‡å­—ã‚’åˆ†é›¢
    tokens = []
    current_token = ""

    for char in text:
        if char.isspace():
            if current_token:
                tokens.append(current_token)
                current_token = ""
        else:
            current_token += char
            # 2-3æ–‡å­—ã”ã¨ã«åŒºåˆ‡ã‚‹ï¼ˆæ—¥æœ¬èªã®å ´åˆï¼‰
            if len(current_token) >= 3 and not char.isascii():
                tokens.append(current_token)
                current_token = ""

    if current_token:
        tokens.append(current_token)

    # çŸ­ã™ãã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å¤–ï¼ˆ1æ–‡å­—ã®ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãªã©ï¼‰
    tokens = [t for t in tokens if len(t) >= 2 or t.isalnum()]

    return tokens


def extract_keywords_llm(query: str, _rag_engine) -> List[str]:
    """
    LLMã‚’ä½¿ç”¨ã—ã¦ã‚¯ã‚¨ãƒªã‹ã‚‰é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã‚’æŠ½å‡º

    Args:
        query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª
        _rag_engine: RAGEngineã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆLLMã‚¢ã‚¯ã‚»ã‚¹ç”¨ï¼‰

    Returns:
        list: æŠ½å‡ºã•ã‚ŒãŸé‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
    """
    if not query or not query.strip():
        logger.warning("Empty query provided for LLM keyword extraction")
        return []

    if not _rag_engine:
        logger.warning("RAGEngine not available for LLM keyword extraction")
        return tokenize_query(query)

    try:
        from langchain_core.messages import HumanMessage

        prompt = f"""ä»¥ä¸‹ã®è³ªå•ã‹ã‚‰ã€PDFãƒšãƒ¼ã‚¸ä¸Šã§ãƒã‚¤ãƒ©ã‚¤ãƒˆã™ã¹ãé‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

**é™¤å¤–ã™ã¹ãã‚‚ã®:**
- åŠ©è©ï¼ˆã®ã€ã¯ã€ã‚’ã€ãŒã€ã«ã€ã§ã€ã¨ã€ã‚„ã€ã‹ã‚‰ã€ã¾ã§ã€ã‚ˆã‚Šã€ã¸ï¼‰
- æŒ‡ç¤ºèªï¼ˆã“ã®ã€ãã®ã€ã‚ã®ã€ã©ã®ã€ã©ã‚Œã€ã„ã¤ã€ã©ã“ï¼‰
- ä¸€èˆ¬çš„ãªå‹•è©ï¼ˆã™ã‚‹ã€ã‚ã‚‹ã€ã„ã‚‹ã€ãªã‚‹ã€è¡Œã†ã€ç¤ºã™ï¼‰
- ç–‘å•è©å˜ä½“ï¼ˆä½•ã€èª°ã€ã„ã¤ã€ã©ã“ã€ãªãœã€ã©ã†ï¼‰
- 1-2æ–‡å­—ã®æ–­ç‰‡ã‚„æ´»ç”¨èªå°¾

**æŠ½å‡ºã™ã¹ãã‚‚ã®:**
- åè©ï¼ˆç‰¹ã«å›ºæœ‰åè©ã€å°‚é–€ç”¨èªã€çµ„ç¹”åã€äººåï¼‰
- é‡è¦ãªå‹•è©ãƒ»å½¢å®¹è©ï¼ˆæ ¸å¿ƒçš„ãªå‹•ä½œã‚„çŠ¶æ…‹ï¼‰
- æ•°å€¤ã‚„æ—¥ä»˜
- è¤‡åˆèªï¼ˆä¾‹: ã€Œå› æœé–¢ä¿‚ã€ã€Œèªå®šå¦èªã€ï¼‰

è³ªå•: {query}

é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜ä¸è¦ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ï¼‰:"""

        # LLMå‘¼ã³å‡ºã—ï¼ˆtemperature=0ã§ç¢ºå®šçš„ãªå‡ºåŠ›ï¼‰
        response = _rag_engine.openai_llm.invoke([HumanMessage(content=prompt)])
        keywords_text = response.content.strip()

        # ã‚«ãƒ³ãƒã¾ãŸã¯ã‚¹ãƒšãƒ¼ã‚¹ã§åˆ†å‰²
        keywords = []
        for k in keywords_text.replace('ã€', ',').split(','):
            k = k.strip()
            if k and len(k) >= 2:  # 1æ–‡å­—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯é™¤å¤–
                keywords.append(k)

        logger.info(f"ğŸ¤– LLM keyword extraction: '{query}' -> {keywords}")
        return keywords if keywords else tokenize_query(query)

    except Exception as e:
        logger.error(f"âŒ LLM keyword extraction failed: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—¢å­˜ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        fallback_keywords = tokenize_query(query)
        logger.info(f"Fallback to tokenization: {fallback_keywords}")
        return fallback_keywords


def find_text_positions(
    pdf_path: Path,
    page_number: int,
    search_terms: List[str],
    vision_analyzer=None,
    dpi: int = DEFAULT_DPI
) -> List[Dict[str, float]]:
    """
    PDFãƒšãƒ¼ã‚¸å†…ã§æŒ‡å®šã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®åº§æ¨™ã‚’æ¤œå‡º

    Args:
        pdf_path: PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        page_number: ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆ1å§‹ã¾ã‚Šï¼‰
        search_terms: æ¤œç´¢ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        vision_analyzer: VisionAnalyzerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆOCRãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã€çœç•¥å¯ï¼‰
        dpi: OCRç”¨ç”»åƒã®è§£åƒåº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 150ï¼‰

    Returns:
        list: åº§æ¨™æƒ…å ±ã®ãƒªã‚¹ãƒˆ
            [{
                "text": str,  # ãƒãƒƒãƒã—ãŸãƒ†ã‚­ã‚¹ãƒˆ
                "x0": float,  # å·¦ç«¯Xåº§æ¨™
                "y0": float,  # ä¸Šç«¯Yåº§æ¨™
                "x1": float,  # å³ç«¯Xåº§æ¨™
                "y1": float,  # ä¸‹ç«¯Yåº§æ¨™
            }]
    """
    positions = []

    if not search_terms:
        return positions

    try:
        with pdfplumber.open(pdf_path) as pdf:
            # ãƒšãƒ¼ã‚¸ç•ªå·ã¯1å§‹ã¾ã‚Šã ãŒã€pdfplumberã¯0å§‹ã¾ã‚Š
            page = pdf.pages[page_number - 1]

            # ãƒšãƒ¼ã‚¸å†…ã®å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªå˜ä½ã§å–å¾—
            words = page.extract_words()

            # ã‚¹ã‚­ãƒ£ãƒ³PDFï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºä¸å¯ï¼‰ã®å ´åˆã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™
            if len(words) == 0:
                logger.warning(f"âš ï¸ PDF page {page_number} has no extractable text (scanned PDF) - no highlights will be shown")
                return []

            # å„æ¤œç´¢èªã«å¯¾ã—ã¦ãƒãƒƒãƒãƒ³ã‚°ã‚’å®Ÿè¡Œï¼ˆæ”¹å–„ç‰ˆï¼šå˜æ–¹å‘éƒ¨åˆ†ä¸€è‡´ + é•·ã•ãƒ•ã‚£ãƒ«ã‚¿ï¼‰
            for search_term in search_terms:
                search_term_lower = search_term.lower()

                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é•·ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ2æ–‡å­—ä»¥ä¸Šã®ã¿ãƒãƒƒãƒãƒ³ã‚°ï¼‰
                if len(search_term_lower) < 2:
                    continue

                for word in words:
                    word_text = word['text'].lower()

                    # å˜æ–¹å‘éƒ¨åˆ†ä¸€è‡´ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå˜èªã«å«ã¾ã‚Œã‚‹å ´åˆã®ã¿ï¼‰
                    if search_term_lower in word_text:
                        positions.append({
                            "text": word['text'],
                            "x0": word['x0'],
                            "y0": word['top'],
                            "x1": word['x1'],
                            "y1": word['bottom'],
                        })

            logger.info(f"ğŸ“ Found {len(positions)} text positions for {len(search_terms)} search terms on page {page_number}")
            if len(positions) == 0 and len(search_terms) > 0:
                logger.warning(f"âš ï¸ No text positions found for search terms: {search_terms}")
            return positions

    except Exception as e:
        logger.error(f"âŒ Error finding text positions: {e}", exc_info=True)
        return []


def highlight_text_on_image(
    image: Image.Image,
    text_positions: List[Dict[str, float]],
    page_height: float,
    dpi: int = DEFAULT_DPI,
    highlight_color: Tuple[int, int, int, int] = (255, 255, 0, 80)  # é»„è‰²åŠé€æ˜
) -> Image.Image:
    """
    ç”»åƒä¸Šã«ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’æç”»

    Args:
        image: å…ƒã®ç”»åƒ
        text_positions: ãƒ†ã‚­ã‚¹ãƒˆåº§æ¨™ã®ãƒªã‚¹ãƒˆ
        page_height: PDFãƒšãƒ¼ã‚¸ã®é«˜ã•ï¼ˆãƒã‚¤ãƒ³ãƒˆå˜ä½ï¼‰
        dpi: ç”»åƒã®DPI
        highlight_color: ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚«ãƒ©ãƒ¼ (R, G, B, Alpha)

    Returns:
        PIL.Image: ãƒã‚¤ãƒ©ã‚¤ãƒˆä»˜ãç”»åƒ
    """
    if not text_positions:
        return image

    try:
        # RGBAãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›ï¼ˆé€éå‡¦ç†ã®ãŸã‚ï¼‰
        if image.mode != 'RGBA':
            image = image.convert('RGBA')

        # é€æ˜ãªã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã‚’ä½œæˆ
        overlay = Image.new('RGBA', image.size, (255, 255, 255, 0))
        draw = ImageDraw.Draw(overlay)

        # PDFåº§æ¨™ã‹ã‚‰ç”»åƒåº§æ¨™ã¸ã®ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°
        # PDFã¯72 DPIã€ç”»åƒã¯DPIæŒ‡å®šã®è§£åƒåº¦
        scale = dpi / 72.0

        # ãƒšãƒ¼ã‚¸é«˜ã•ã®ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´
        img_height = image.height

        # å„ãƒ†ã‚­ã‚¹ãƒˆä½ç½®ã«ãƒã‚¤ãƒ©ã‚¤ãƒˆçŸ©å½¢ã‚’æç”»
        for pos in text_positions:
            # PDFåº§æ¨™ï¼ˆåŸç‚¹ã¯å·¦ä¸‹ï¼‰ã‹ã‚‰ç”»åƒåº§æ¨™ï¼ˆåŸç‚¹ã¯å·¦ä¸Šï¼‰ã«å¤‰æ›
            x0 = pos['x0'] * scale
            y0 = (page_height - pos['y1']) * scale  # Yåº§æ¨™ã¯åè»¢
            x1 = pos['x1'] * scale
            y1 = (page_height - pos['y0']) * scale

            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¿½åŠ ï¼ˆèª­ã¿ã‚„ã™ãã™ã‚‹ãŸã‚ï¼‰
            padding = 2
            draw.rectangle(
                [(x0 - padding, y0 - padding), (x1 + padding, y1 + padding)],
                fill=highlight_color
            )

        # ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã‚’å…ƒç”»åƒã«åˆæˆ
        highlighted = Image.alpha_composite(image, overlay)

        # RGBãƒ¢ãƒ¼ãƒ‰ã«æˆ»ã™ï¼ˆStreamlitã§ã®è¡¨ç¤ºã®ãŸã‚ï¼‰
        highlighted = highlighted.convert('RGB')

        logger.info(f"Applied {len(text_positions)} highlights to image")
        return highlighted

    except Exception as e:
        logger.error(f"Error highlighting text on image: {e}", exc_info=True)
        return image


@st.cache_data(ttl=3600, show_spinner=False, max_entries=100)
def extract_page_with_highlight(
    source_file: str,
    page_number: int,
    query: str,
    _vector_store,
    _rag_engine=None,
    _vision_analyzer=None,
    use_llm_keywords: bool = True,
    cache_version: int = 4,  # v4: OCR fallback implementation (NO underscore to include in cache key!)
    dpi: int = DEFAULT_DPI,
    target_width: int = DEFAULT_WIDTH
) -> Optional[Image.Image]:
    """
    PDFãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ›ã—ã€æ¤œç´¢ã‚¯ã‚¨ãƒªã«ä¸€è‡´ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ

    Args:
        source_file: PDFãƒ•ã‚¡ã‚¤ãƒ«å
        page_number: ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆ1å§‹ã¾ã‚Šï¼‰
        query: æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆãƒã‚¤ãƒ©ã‚¤ãƒˆå¯¾è±¡ï¼‰
        _vector_store: VectorStoreã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        _rag_engine: RAGEngineã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆLLMã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã«ä½¿ç”¨ã€çœç•¥å¯ï¼‰
        _vision_analyzer: VisionAnalyzerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆOCRãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã€çœç•¥å¯ï¼‰
        use_llm_keywords: LLMã‚’ä½¿ç”¨ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚’æœ‰åŠ¹åŒ–ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
        cache_version: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆå¤‰æ›´æ™‚ã«ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆã€é€šå¸¸å¤‰æ›´ä¸è¦ï¼‰
        dpi: è§£åƒåº¦
        target_width: ç”»åƒå¹…

    Returns:
        PIL.Image: ãƒã‚¤ãƒ©ã‚¤ãƒˆä»˜ãç”»åƒã€å¤±æ•—æ™‚ã¯None
    """
    # ğŸ” å®Ÿè¡Œç¢ºèªãƒ­ã‚°ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    logger.info(f"ğŸ“¸ extract_page_with_highlight() CALLED - cache_v{cache_version}, pdf2image={PDF2IMAGE_AVAILABLE}")
    logger.info(f"   â†’ source={source_file}, page={page_number}, query_len={len(query) if query else 0}")

    if not PDF2IMAGE_AVAILABLE:
        logger.warning("=" * 60)
        logger.warning("âŒ PDF page rendering is DISABLED (poppler not installed)")
        logger.warning(f"âŒ Cannot render page {page_number} of {source_file}")
        logger.warning("ğŸ’¡ Check Streamlit Cloud logs for poppler installation errors")
        logger.warning("=" * 60)
        return None

    try:
        # PDFã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
        pdf_path = get_pdf_path(source_file, _vector_store)
        if not pdf_path:
            logger.error(f"Failed to get PDF path: {source_file}")
            return None

        # ã‚¯ã‚¨ãƒªã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆLLM or ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼‰
        logger.info(f"ğŸ” Highlighting query: '{query}' for {source_file} page {page_number}")

        if query:
            # LLMãƒ™ãƒ¼ã‚¹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆæ¨å¥¨ï¼‰
            if use_llm_keywords and _rag_engine is not None:
                try:
                    search_terms = extract_keywords_llm(query, _rag_engine)
                    logger.info(f"ğŸ¤– LLM-extracted keywords: {search_terms}")
                except Exception as e:
                    logger.warning(f"âš ï¸ LLM keyword extraction failed: {e}, falling back to tokenization")
                    search_terms = tokenize_query(query)
                    logger.info(f"ğŸ”¤ Fallback tokenized keywords: {search_terms}")
            else:
                # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                search_terms = tokenize_query(query)
                logger.info(f"ğŸ”¤ Tokenized keywords: {search_terms}")
        else:
            search_terms = []

        # ãƒ†ã‚­ã‚¹ãƒˆä½ç½®ã‚’æ¤œå‡º
        text_positions = []
        page_height = 0

        if search_terms:
            with pdfplumber.open(pdf_path) as pdf:
                page = pdf.pages[page_number - 1]
                page_height = page.height
                text_positions = find_text_positions(pdf_path, page_number, search_terms, _vision_analyzer, dpi)
                logger.info(f"ğŸ“Š Text positions found: {len(text_positions)}")
        else:
            logger.warning("âš ï¸ No search terms to highlight")

        # ãƒšãƒ¼ã‚¸ã‚’ç”»åƒã«å¤‰æ›
        images = convert_from_path(
            str(pdf_path),
            dpi=dpi,
            first_page=page_number,
            last_page=page_number,
            fmt='png'
        )

        if not images or len(images) == 0:
            logger.error(f"No image generated for page {page_number} of {source_file}")
            return None

        image = images[0]

        # ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’é©ç”¨
        if text_positions and page_height > 0:
            image = highlight_text_on_image(image, text_positions, page_height, dpi)
            logger.info(f"âœ… Applied {len(text_positions)} highlights to page {page_number} of {source_file}")
        elif search_terms:
            logger.warning(f"âš ï¸ No highlights applied (text_positions={len(text_positions)}, page_height={page_height})")

        # ç”»åƒã‚µã‚¤ã‚ºã‚’èª¿æ•´
        if image.width > target_width:
            aspect_ratio = image.height / image.width
            new_height = int(target_width * aspect_ratio)
            image = image.resize((target_width, new_height), Image.Resampling.LANCZOS)

        return image

    except Exception as e:
        logger.error(f"Error extracting page with highlight: {e}", exc_info=True)
        return None
