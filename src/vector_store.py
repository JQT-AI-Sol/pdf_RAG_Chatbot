"""
Vector store module supporting both ChromaDB and Supabase
"""

import logging
import os
from typing import List, Dict, Any, Optional
import uuid
import hashlib

logger = logging.getLogger(__name__)


class VectorStore:
    """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚¯ãƒ©ã‚¹ï¼ˆChromaDB / Supabaseå¯¾å¿œï¼‰"""

    def __init__(self, config: dict):
        """
        åˆæœŸåŒ–

        Args:
            config: è¨­å®šè¾æ›¸
        """
        self.config = config
        self.vs_config = config.get('vector_store', {})
        self.provider = self.vs_config.get('provider', 'chromadb')

        if self.provider == 'supabase':
            self._init_supabase()
        else:
            self._init_chromadb()

        # BM25ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ç”¨ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–
        self._init_tokenizer()

        logger.info(f"Vector store initialized with provider: {self.provider} (v1.1)")

    def _init_supabase(self):
        """Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–"""
        try:
            from supabase import create_client, Client

            # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰æ¥ç¶šæƒ…å ±ã‚’å–å¾—
            supabase_url = os.environ.get('SUPABASE_URL')
            supabase_key = os.environ.get('SUPABASE_KEY')

            if not supabase_url or not supabase_key:
                raise ValueError("SUPABASE_URL and SUPABASE_KEY must be set in environment variables")

            self.client: Client = create_client(supabase_url, supabase_key)

            # ãƒ†ãƒ¼ãƒ–ãƒ«å
            supabase_config = self.vs_config.get('supabase', {})
            self.text_table = supabase_config.get('table_name_text', 'pdf_text_chunks')
            self.image_table = supabase_config.get('table_name_images', 'pdf_image_contents')
            self.pdf_table = supabase_config.get('table_name_pdfs', 'registered_pdfs')
            self.match_threshold = supabase_config.get('match_threshold', 0.7)
            self.storage_bucket = supabase_config.get('storage_bucket', 'pdf-images')
            self.pdf_storage_bucket = supabase_config.get('pdf_storage_bucket', 'pdf-files')

            logger.info(f"Supabase client initialized (URL: {supabase_url})")

            # Storageãƒã‚±ãƒƒãƒˆã®ç¢ºèªãƒ»ä½œæˆ
            try:
                # ãƒã‚±ãƒƒãƒˆãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                buckets = self.client.storage.list_buckets()
                bucket_names = [b.name for b in buckets]

                # ç”»åƒç”¨ãƒã‚±ãƒƒãƒˆ
                if self.storage_bucket not in bucket_names:
                    # ãƒã‚±ãƒƒãƒˆãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
                    self.client.storage.create_bucket(
                        self.storage_bucket,
                        options={"public": False}  # ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒã‚±ãƒƒãƒˆ
                    )
                    logger.info(f"Created Supabase Storage bucket: {self.storage_bucket}")
                else:
                    logger.info(f"Using existing Supabase Storage bucket: {self.storage_bucket}")

                # PDFç”¨ãƒã‚±ãƒƒãƒˆ
                if self.pdf_storage_bucket not in bucket_names:
                    self.client.storage.create_bucket(
                        self.pdf_storage_bucket,
                        options={"public": False}  # ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒã‚±ãƒƒãƒˆ
                    )
                    logger.info(f"Created Supabase Storage bucket for PDFs: {self.pdf_storage_bucket}")
                else:
                    logger.info(f"Using existing Supabase Storage bucket for PDFs: {self.pdf_storage_bucket}")
            except Exception as e:
                logger.warning(f"Could not verify/create storage bucket: {e}. Continuing anyway...")

        except Exception as e:
            logger.error(f"Failed to initialize Supabase: {e}")
            raise

    def _init_chromadb(self):
        """ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–"""
        import chromadb

        chroma_config = self.vs_config.get('chromadb', {})

        # Streamlit Cloudç’°å¢ƒã‚’æ¤œå‡º
        is_streamlit_cloud = (
            os.environ.get('STREAMLIT_RUNTIME_ENV') == 'cloud' or
            os.path.exists('/mount/src') or
            'STREAMLIT_SHARING_MODE' in os.environ
        )

        if is_streamlit_cloud:
            persist_dir = '/tmp/chroma_db'
            logger.warning(f"Running on Streamlit Cloud - using PersistentClient with temporary directory: {persist_dir}")
            os.makedirs(persist_dir, exist_ok=True)
            self.client = chromadb.PersistentClient(path=persist_dir)
        else:
            self.client = chromadb.PersistentClient(
                path=chroma_config.get('persist_directory', './data/chroma_db')
            )

        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
        self.text_collection_name = chroma_config.get('collection_name_text', 'pdf_text_chunks')
        self.image_collection_name = chroma_config.get('collection_name_images', 'pdf_image_contents')

        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—ã¾ãŸã¯ä½œæˆ
        self.text_collection = self.client.get_or_create_collection(
            name=self.text_collection_name
        )
        self.image_collection = self.client.get_or_create_collection(
            name=self.image_collection_name
        )

    def _init_tokenizer(self):
        """
        BM25ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ç”¨ã®æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–
        """
        try:
            import MeCab
            self.tokenizer = MeCab.Tagger("-Owakati")
            logger.info("MeCab tokenizer initialized for BM25 hybrid search")
        except ImportError:
            logger.warning("MeCab not available, using simple space-based tokenization as fallback")
            self.tokenizer = None
        except Exception as e:
            logger.warning(f"Failed to initialize MeCab: {e}. Using simple tokenization as fallback")
            self.tokenizer = None

    def _tokenize(self, text: str) -> List[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰

        Args:
            text: ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            list: ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ
        """
        if not text:
            return []

        if self.tokenizer:
            # MeCabã§åˆ†ã‹ã¡æ›¸ã
            try:
                tokens = self.tokenizer.parse(text).strip().split()
                logger.debug(f"MeCab tokenization: '{text}' -> {tokens[:10]}{'...' if len(tokens) > 10 else ''}")
                return [token for token in tokens if len(token) > 1]  # 1æ–‡å­—ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯é™¤å¤–
            except Exception as e:
                logger.warning(f"MeCab tokenization failed: {e}, falling back to regex-based split")

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Regex-based smart tokenization
        # è‹±æ•°å­—ã€ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ã‚’ãã‚Œãã‚Œã¾ã¨ã¾ã‚Šã¨ã—ã¦æŠ½å‡º
        import re

        # é‡è¦ãªç•¥èªï¼ˆ2-4æ–‡å­—ã®è‹±èªï¼‰ã‚’å…ˆã«ä¿è­·
        important_keywords = ['SNS', 'AI', 'IT', 'PC', 'OS', 'API', 'URL', 'VPN', 'DNS', 'HTTP', 'HTTPS']

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã§å˜èªã‚’æŠ½å‡º
        # [a-zA-Z0-9]+: é€£ç¶šã™ã‚‹è‹±æ•°å­—ï¼ˆSNSã€APIã€123ãªã©ï¼‰
        # [ã-ã‚“]+: é€£ç¶šã™ã‚‹ã²ã‚‰ãŒãªï¼ˆåˆ©ç”¨ã€æ³¨æ„ãªã©ï¼‰
        # [ã‚¡-ãƒ´ãƒ¼]+: é€£ç¶šã™ã‚‹ã‚«ã‚¿ã‚«ãƒŠï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãªã©ï¼‰
        # [ä¸€-é¾¥]+: é€£ç¶šã™ã‚‹æ¼¢å­—ï¼ˆæ³¨æ„ç‚¹ã€åˆ©ç”¨ãªã©ï¼‰
        words = re.findall(r'[a-zA-Z0-9]+|[ã-ã‚“]+|[ã‚¡-ãƒ´ãƒ¼]+|[ä¸€-é¾¥]+', text)

        tokens = []
        for word in words:
            # è‹±æ•°å­—ã®å ´åˆ
            if word.isascii():
                # é‡è¦ãªç•¥èªã¯å¤§æ–‡å­—ã§ä¿æŒ
                upper_word = word.upper()
                if upper_word in important_keywords:
                    tokens.append(upper_word)
                # 2æ–‡å­—ä»¥ä¸Šã®è‹±æ•°å­—ã¯å°æ–‡å­—åŒ–ã—ã¦è¿½åŠ 
                elif len(word) >= 2:
                    tokens.append(word.lower())
            # æ—¥æœ¬èªã®å ´åˆã¯2æ–‡å­—ä»¥ä¸Šã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿
            elif len(word) >= 2:
                tokens.append(word)

        logger.debug(f"Regex tokenization: '{text}' -> {tokens[:10]}{'...' if len(tokens) > 10 else ''}")
        return tokens

    def add_text_chunks(self, chunks: List[Dict[str, Any]], embeddings: List[List[float]]):
        """
        ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«è¿½åŠ 

        Args:
            chunks: ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
            embeddings: å¯¾å¿œã™ã‚‹ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã®ãƒªã‚¹ãƒˆ
        """
        if self.provider == 'supabase':
            self._add_text_chunks_supabase(chunks, embeddings)
        else:
            self._add_text_chunks_chromadb(chunks, embeddings)

    def _add_text_chunks_supabase(self, chunks: List[Dict[str, Any]], embeddings: List[List[float]]):
        """Supabaseã«ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ """
        try:
            # ãƒ‡ãƒãƒƒã‚°: Embeddingã‚µã‚¤ã‚ºã‚’ç¢ºèª
            if embeddings and len(embeddings) > 0:
                first_emb = embeddings[0]
                first_emb_dim = len(first_emb)
                logger.info(f"ğŸ” DEBUG: Saving {len(embeddings)} embeddings, first dimension: {first_emb_dim}")
                logger.info(f"ğŸ” DEBUG: Embedding type before save: {type(first_emb)}")
                logger.info(f"ğŸ” DEBUG: First 3 values: {first_emb[:3]}")
                if first_emb_dim != 3072:
                    logger.error(f"âŒ DEBUG: ABNORMAL embedding dimension before save! Expected 3072, got {first_emb_dim}")

            records = []
            for chunk, embedding in zip(chunks, embeddings):
                records.append({
                    'id': f"text_{uuid.uuid4().hex[:16]}",
                    'content': chunk['text'],
                    'embedding': embedding,  # List[float]ã®ã¾ã¾æ¸¡ã™ï¼ˆSupabaseãŒè‡ªå‹•å¤‰æ›ï¼‰
                    'source_file': chunk['source_file'],
                    'page_number': chunk['page_number'],
                    'category': chunk['category'],
                    'content_type': 'text'
                })

            self.client.table(self.text_table).insert(records).execute()
            logger.info(f"Added {len(chunks)} text chunks to Supabase")

        except Exception as e:
            logger.error(f"Error adding text chunks to Supabase: {e}")
            raise

    def _add_text_chunks_chromadb(self, chunks: List[Dict[str, Any]], embeddings: List[List[float]]):
        """ChromaDBã«ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ """
        try:
            ids = [f"text_{uuid.uuid4().hex[:16]}_{i}" for i in range(len(chunks))]
            documents = [chunk['text'] for chunk in chunks]
            metadatas = [
                {
                    'source_file': chunk['source_file'],
                    'page_number': chunk['page_number'],
                    'category': chunk['category'],
                    'content_type': 'text'
                }
                for chunk in chunks
            ]

            self.text_collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas
            )

            logger.info(f"Added {len(chunks)} text chunks to ChromaDB")

        except Exception as e:
            logger.error(f"Error adding text chunks to ChromaDB: {e}")
            raise

    def add_image_contents_batch(self, image_data_list: List[Dict[str, Any]], embeddings: List[List[float]]):
        """
        è¤‡æ•°ã®ç”»åƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒãƒƒãƒã§ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«è¿½åŠ 

        Args:
            image_data_list: ç”»åƒãƒ‡ãƒ¼ã‚¿ã¨è§£æçµæœã®ãƒªã‚¹ãƒˆ
            embeddings: å¯¾å¿œã™ã‚‹ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã®ãƒªã‚¹ãƒˆ
        """
        if not image_data_list or not embeddings:
            return

        if self.provider == 'supabase':
            self._add_image_contents_supabase(image_data_list, embeddings)
        else:
            self._add_image_contents_chromadb(image_data_list, embeddings)

    def _add_image_contents_supabase(self, image_data_list: List[Dict[str, Any]], embeddings: List[List[float]]):
        """Supabaseã«ç”»åƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿½åŠ ï¼ˆç”»åƒã¯Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰"""
        try:
            from pathlib import Path

            # ãƒ‡ãƒãƒƒã‚°: Embeddingã‚µã‚¤ã‚ºã‚’ç¢ºèª
            if embeddings and len(embeddings) > 0:
                first_emb_dim = len(embeddings[0])
                logger.info(f"ğŸ” DEBUG: Saving {len(embeddings)} image embeddings, first dimension: {first_emb_dim}")
                if first_emb_dim != 3072:
                    logger.error(f"âŒ DEBUG: ABNORMAL image embedding dimension before save! Expected 3072, got {first_emb_dim}")

            records = []
            for img_data, embedding in zip(image_data_list, embeddings):
                image_id = f"image_{hashlib.md5(img_data['image_path'].encode()).hexdigest()}"

                # ç”»åƒã‚’Supabase Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                local_image_path = img_data['image_path']
                storage_path = None

                if Path(local_image_path).exists():
                    try:
                        # Storageãƒ‘ã‚¹ã‚’ç”Ÿæˆï¼ˆURL-safeå½¢å¼ï¼‰
                        # æ—¥æœ¬èªã‚’å«ã‚€ã‚«ãƒ†ã‚´ãƒªãƒ¼åãƒ»ãƒ•ã‚¡ã‚¤ãƒ«åã¯Supabase Storageã§ä½¿ãˆãªã„ãŸã‚ã€
                        # ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¹ã‚’ä½¿ç”¨
                        category = img_data.get('category', 'uncategorized')
                        filename = Path(local_image_path).name

                        # ã‚«ãƒ†ã‚´ãƒªãƒ¼ã¨ãƒ•ã‚¡ã‚¤ãƒ«åã‚’URL-safeã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                        # ã•ã‚‰ã«ãƒãƒƒã‚·ãƒ¥ã‚’ä»˜ã‘ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯æ€§ã‚’ä¿è¨¼
                        category_hash = hashlib.md5(category.encode('utf-8')).hexdigest()[:8]
                        file_ext = Path(filename).suffix
                        file_hash = hashlib.md5(filename.encode('utf-8')).hexdigest()[:16]
                        storage_path = f"cat_{category_hash}/img_{file_hash}{file_ext}"

                        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                        with open(local_image_path, 'rb') as f:
                            image_bytes = f.read()

                        self.client.storage.from_(self.storage_bucket).upload(
                            storage_path,
                            image_bytes,
                            file_options={"content-type": "image/png", "upsert": "true"}
                        )
                        logger.debug(f"Uploaded image to Storage: {storage_path}")

                    except Exception as upload_error:
                        logger.warning(f"Failed to upload image {local_image_path} to Storage: {upload_error}")
                        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—æ™‚ã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’ãã®ã¾ã¾ä½¿ç”¨
                        storage_path = local_image_path
                else:
                    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’ãã®ã¾ã¾ä½¿ç”¨
                    logger.warning(f"Image file not found: {local_image_path}")
                    storage_path = local_image_path

                records.append({
                    'id': image_id,
                    'content': img_data.get('description', ''),
                    'embedding': embedding,  # List[float]ã®ã¾ã¾æ¸¡ã™ï¼ˆSupabaseãŒè‡ªå‹•å¤‰æ›ï¼‰
                    'source_file': img_data.get('source_file', ''),
                    'page_number': img_data.get('page_number', 0),
                    'category': img_data.get('category', ''),
                    'content_type': img_data.get('content_type', 'image'),
                    'image_path': storage_path  # Storage pathã‚’ä¿å­˜
                })

            # upsertã§on_conflictã‚’æ˜ç¤ºçš„ã«æŒ‡å®šï¼ˆä¸»ã‚­ãƒ¼idã§ç«¶åˆè§£æ±ºï¼‰
            self.client.table(self.image_table).upsert(
                records,
                on_conflict='id'
            ).execute()
            logger.info(f"Upserted {len(image_data_list)} image contents to Supabase")

        except Exception as e:
            logger.error(f"Error adding image contents to Supabase: {e}")
            raise

    def _add_image_contents_chromadb(self, image_data_list: List[Dict[str, Any]], embeddings: List[List[float]]):
        """ChromaDBã«ç”»åƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿½åŠ """
        try:
            ids = [f"image_{hashlib.md5(img_data['image_path'].encode()).hexdigest()}" for img_data in image_data_list]
            documents = [img_data.get('description', '') for img_data in image_data_list]
            metadatas = [
                {
                    'source_file': img_data.get('source_file', ''),
                    'page_number': img_data.get('page_number', 0),
                    'category': img_data.get('category', ''),
                    'content_type': img_data.get('content_type', 'image'),
                    'image_path': img_data['image_path']
                }
                for img_data in image_data_list
            ]

            self.image_collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas
            )

            logger.info(f"Added {len(image_data_list)} image contents to ChromaDB")

        except Exception as e:
            logger.error(f"Error adding image contents to ChromaDB: {e}")
            raise

    def search(self, query_embedding: List[float], category: Optional[str] = None,
               top_k: int = 5, search_type: str = 'both', query_text: Optional[str] = None) -> Dict[str, List[Dict[str, Any]]]:
        """
        ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆBM25ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢å¯¾å¿œï¼‰

        Args:
            query_embedding: ã‚¯ã‚¨ãƒªã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°
            category: æ¤œç´¢å¯¾è±¡ã‚«ãƒ†ã‚´ãƒªãƒ¼ï¼ˆNoneã®å ´åˆã¯å…¨ã‚«ãƒ†ã‚´ãƒªãƒ¼ï¼‰
            top_k: å–å¾—ã™ã‚‹çµæœã®æ•°
            search_type: æ¤œç´¢ã‚¿ã‚¤ãƒ— ('text', 'image', 'both')
            query_text: ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆï¼ˆBM25ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ç”¨ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            dict: æ¤œç´¢çµæœï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒï¼‰
        """
        # BM25ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®æœ‰åŠ¹åŒ–ã‚’ç¢ºèª
        hybrid_config = self.config.get('hybrid_search', {})
        use_hybrid = (
            hybrid_config.get('enabled', False) and
            query_text and
            self.provider == 'supabase' and
            search_type in ['text', 'both']
        )

        if use_hybrid:
            logger.info("Using BM25 hybrid search for text results")
            return self._hybrid_search_supabase(query_text, query_embedding, category, top_k, search_type)
        else:
            # å¾“æ¥ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿
            if self.provider == 'supabase':
                return self._search_supabase(query_embedding, category, top_k, search_type)
            else:
                return self._search_chromadb(query_embedding, category, top_k, search_type)

    def _reciprocal_rank_fusion(self, vector_results: List[Dict], bm25_results: List[Dict],
                               alpha: float = 0.7, k: int = 60) -> List[Dict]:
        """
        Reciprocal Rank Fusion (RRF) ã§ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨BM25ã®çµæœã‚’çµ±åˆ

        Args:
            vector_results: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
            bm25_results: BM25æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆï¼ˆã‚¹ã‚³ã‚¢ä»˜ãï¼‰
            alpha: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®é‡ã¿ï¼ˆ0-1ï¼‰ã€BM25ã®é‡ã¿ã¯1-alpha
            k: RRFãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ60ï¼‰

        Returns:
            list: çµ±åˆã•ã‚ŒãŸæ¤œç´¢çµæœï¼ˆã‚¹ã‚³ã‚¢é †ï¼‰
        """
        # IDã§ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«è¾æ›¸åŒ–
        vector_dict = {r['id']: (i, r) for i, r in enumerate(vector_results)}
        bm25_dict = {r['id']: (i, r) for i, r in enumerate(bm25_results)}

        # å…¨ã¦ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªIDã‚’å–å¾—
        all_ids = set(vector_dict.keys()) | set(bm25_dict.keys())

        # RRFã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        fused_results = []
        for doc_id in all_ids:
            score = 0.0

            # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ãƒ©ãƒ³ã‚¯ã‹ã‚‰ã‚¹ã‚³ã‚¢è¨ˆç®—
            if doc_id in vector_dict:
                vector_rank, vector_result = vector_dict[doc_id]
                score += alpha * (1.0 / (k + vector_rank + 1))
                result_data = vector_result
            else:
                result_data = None

            # BM25ã®ãƒ©ãƒ³ã‚¯ã‹ã‚‰ã‚¹ã‚³ã‚¢è¨ˆç®—
            if doc_id in bm25_dict:
                bm25_rank, bm25_result = bm25_dict[doc_id]
                score += (1 - alpha) * (1.0 / (k + bm25_rank + 1))
                if result_data is None:
                    result_data = bm25_result

            # çµæœã‚’è¿½åŠ 
            if result_data:
                result_with_score = result_data.copy()
                result_with_score['hybrid_score'] = score
                fused_results.append(result_with_score)

        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        fused_results.sort(key=lambda x: x['hybrid_score'], reverse=True)

        logger.debug(f"RRF fusion: {len(vector_results)} vector + {len(bm25_results)} BM25 â†’ {len(fused_results)} merged results")
        if fused_results:
            top_scores = [r['hybrid_score'] for r in fused_results[:3]]
            logger.info(f"Top 3 hybrid scores: {top_scores}")

        return fused_results

    def _hybrid_search_supabase(self, query_text: str, query_embedding: List[float],
                               category: Optional[str], top_k: int, search_type: str) -> Dict[str, List[Dict[str, Any]]]:
        """
        BM25 + ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆSupabaseï¼‰

        Args:
            query_text: ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆ
            query_embedding: ã‚¯ã‚¨ãƒªã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°
            category: æ¤œç´¢å¯¾è±¡ã‚«ãƒ†ã‚´ãƒªãƒ¼
            top_k: æœ€çµ‚çš„ã«è¿”ã™çµæœã®æ•°
            search_type: æ¤œç´¢ã‚¿ã‚¤ãƒ— ('text', 'image', 'both')

        Returns:
            dict: æ¤œç´¢çµæœï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒï¼‰
        """
        from rank_bm25 import BM25Okapi

        results = {'text': [], 'images': []}

        try:
            # === 1. ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼‰ ===
            if search_type in ['text', 'both']:
                # 1.1 å…¨å€™è£œã‚’å–å¾—ï¼ˆã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ã‚ã‚Šã€thresholdãªã—ï¼‰
                query_builder = self.client.table(self.text_table).select('id, content, source_file, page_number, category')
                if category:
                    query_builder = query_builder.eq('category', category)

                all_candidates_response = query_builder.execute()

                if not all_candidates_response.data:
                    logger.warning(f"No text candidates found for category: {category}")
                else:
                    all_candidates = all_candidates_response.data
                    logger.info(f"Retrieved {len(all_candidates)} text candidates for hybrid search")

                    # 1.2 ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼ˆä¸Šä½å€™è£œã‚’å¤šã‚ã«å–å¾—ï¼‰
                    top_k_vector = min(top_k * 3, len(all_candidates))  # top_kã®3å€ï¼ˆæœ€å¤§ã§å…¨å€™è£œï¼‰
                    vector_results_dict = self._search_supabase(
                        query_embedding, category, top_k_vector, 'text'
                    )
                    vector_results = vector_results_dict.get('text', [])
                    logger.info(f"Vector search returned {len(vector_results)} results")

                    # 1.3 BM25æ¤œç´¢
                    # å€™è£œã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
                    corpus = [candidate['content'] for candidate in all_candidates]
                    tokenized_corpus = [self._tokenize(doc) for doc in corpus]

                    # BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
                    bm25 = BM25Okapi(tokenized_corpus)

                    # ã‚¯ã‚¨ãƒªã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦BM25ã‚¹ã‚³ã‚¢è¨ˆç®—
                    tokenized_query = self._tokenize(query_text)
                    bm25_scores = bm25.get_scores(tokenized_query)

                    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
                    bm25_ranked_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)

                    # ä¸Šä½å€™è£œã‚’å–å¾—
                    top_k_bm25 = min(top_k * 3, len(all_candidates))
                    bm25_results = []
                    for idx in bm25_ranked_indices[:top_k_bm25]:
                        candidate = all_candidates[idx]
                        bm25_results.append({
                            'id': candidate['id'],
                            'content': candidate['content'],
                            'source_file': candidate['source_file'],
                            'page_number': candidate['page_number'],
                            'category': candidate['category'],
                            'content_type': 'text',
                            'bm25_score': float(bm25_scores[idx]),
                            'metadata': {
                                'source_file': candidate['source_file'],
                                'page_number': candidate['page_number'],
                                'category': candidate['category'],
                                'content_type': 'text'
                            }
                        })

                    logger.info(f"BM25 search returned {len(bm25_results)} results (top score: {bm25_results[0]['bm25_score'] if bm25_results else 0:.2f})")

                    # 1.4 Reciprocal Rank Fusion (RRF)
                    hybrid_config = self.config.get('hybrid_search', {})
                    alpha = hybrid_config.get('alpha', 0.7)

                    fused_results = self._reciprocal_rank_fusion(vector_results, bm25_results, alpha=alpha)

                    # ä¸Šä½top_kä»¶ã‚’è¿”ã™
                    results['text'] = fused_results[:top_k]

                    logger.info(f"Hybrid search completed: {len(results['text'])} final text results")

            # === 2. ç”»åƒæ¤œç´¢ï¼ˆå¾“æ¥ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿ï¼‰ ===
            if search_type in ['image', 'both']:
                image_results_dict = self._search_supabase(query_embedding, category, top_k, 'image')
                results['images'] = image_results_dict.get('images', [])

        except Exception as e:
            logger.error(f"Error during hybrid search: {e}", exc_info=True)
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å¾“æ¥ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            logger.warning("Falling back to vector-only search")
            return self._search_supabase(query_embedding, category, top_k, search_type)

        return results

    def _search_supabase(self, query_embedding: List[float], category: Optional[str],
                        top_k: int, search_type: str) -> Dict[str, List[Dict[str, Any]]]:
        """Supabaseã§ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢"""
        results = {'text': [], 'images': []}

        try:
            # ãƒ‡ãƒãƒƒã‚°: ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            if category:
                count_response = self.client.table(self.text_table)\
                    .select('id', count='exact')\
                    .eq('category', category)\
                    .execute()
                logger.info(f"ğŸ” DEBUG: Found {count_response.count} text chunks with category='{category}' in database")

            # ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢
            if search_type in ['text', 'both']:
                logger.info(f"Calling match_text_chunks with category={category}, top_k={top_k}, threshold={self.match_threshold}")

                # ğŸ” ãƒ‡ãƒãƒƒã‚°: query_embeddingã‚’ç¢ºèª
                logger.info(f"ğŸ” DEBUG: query_embedding type={type(query_embedding)}, len={len(query_embedding) if query_embedding else 0}")
                if query_embedding and len(query_embedding) > 0:
                    logger.info(f"ğŸ” DEBUG: First 3 values: {query_embedding[:3]}")
                else:
                    logger.error(f"âŒ DEBUG: query_embedding is empty or None!")

                # RPCãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æº–å‚™
                rpc_params = {
                    'query_embedding': query_embedding,  # List[float]ã®ã¾ã¾æ¸¡ã™ï¼ˆSupabaseãŒè‡ªå‹•å¤‰æ›ï¼‰
                    'match_threshold': self.match_threshold,
                    'match_count': top_k,
                    'filter_category': category
                }

                logger.info(f"ğŸ” DEBUG: RPC params prepared - threshold={self.match_threshold}, count={top_k}, category={category}")

                response = self.client.rpc('match_text_chunks', rpc_params).execute()

                logger.info(f"Text search response received: {len(response.data) if response.data else 0} results")

                if response.data and len(response.data) > 0:
                    logger.info(f"Supabase text result - Keys: {list(response.data[0].keys())}")
                    logger.info(f"Supabase text result - Sample data: source_file={response.data[0].get('source_file')}, page={response.data[0].get('page_number')}, category={response.data[0].get('category')}")

                if response.data:
                    results['text'] = [
                        {
                            'id': row.get('id', ''),
                            'content': row.get('content', ''),
                            'source_file': row.get('source_file', ''),
                            'page_number': row.get('page_number', 0),
                            'category': row.get('category', ''),
                            'content_type': 'text',  # ãƒ†ã‚­ã‚¹ãƒˆã¯å¸¸ã«'text'
                            'distance': row.get('distance', 1 - row.get('similarity', 0)),
                            'metadata': {
                                'source_file': row.get('source_file', ''),
                                'page_number': row.get('page_number', 0),
                                'category': row.get('category', ''),
                                'content_type': 'text'
                            }
                        }
                        for row in response.data
                    ]

            # ç”»åƒæ¤œç´¢
            if search_type in ['image', 'both']:
                # ãƒ‡ãƒãƒƒã‚°: ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                if category:
                    count_response = self.client.table(self.image_table)\
                        .select('id', count='exact')\
                        .eq('category', category)\
                        .execute()
                    logger.info(f"ğŸ” DEBUG: Found {count_response.count} image contents with category='{category}' in database")

                logger.info(f"Calling match_image_contents with category={category}, top_k={top_k}, threshold={self.match_threshold}")

                response = self.client.rpc(
                    'match_image_contents',
                    {
                        'query_embedding': query_embedding,  # List[float]ã®ã¾ã¾æ¸¡ã™ï¼ˆSupabaseãŒè‡ªå‹•å¤‰æ›ï¼‰
                        'match_threshold': self.match_threshold,
                        'match_count': top_k,
                        'filter_category': category
                    }
                ).execute()

                logger.info(f"Image search response received: {len(response.data) if response.data else 0} results")

                # ãƒ‡ãƒãƒƒã‚°: å®Ÿéš›ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç¢ºèª
                if response.data and len(response.data) > 0:
                    logger.info(f"Supabase image result - Keys: {list(response.data[0].keys())}")
                    logger.info(f"Supabase image result - Sample data: source_file={response.data[0].get('source_file')}, page={response.data[0].get('page_number')}, category={response.data[0].get('category')}")
                else:
                    logger.warning("No image results returned from Supabase RPC")

                if response.data:
                    results['images'] = [
                        {
                            'id': row.get('id', ''),
                            'description': row.get('content', ''),
                            'source_file': row.get('source_file', ''),
                            'page_number': row.get('page_number', 0),
                            'category': row.get('category', ''),
                            'content_type': row.get('content_type', 'image'),  # DBã‹ã‚‰å–å¾—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯'image'
                            'path': row.get('image_path', ''),
                            'distance': row.get('distance', 1 - row.get('similarity', 0)),
                            'metadata': {
                                'source_file': row.get('source_file', ''),
                                'page_number': row.get('page_number', 0),
                                'category': row.get('category', ''),
                                'content_type': row.get('content_type', 'image')
                            }
                        }
                        for row in response.data
                    ]

            logger.info(f"Search completed: {len(results['text'])} text, {len(results['images'])} images")

        except Exception as e:
            logger.error(f"Error during Supabase search: {e}")
            raise

        return results

    def _search_chromadb(self, query_embedding: List[float], category: Optional[str],
                        top_k: int, search_type: str) -> Dict[str, List[Dict[str, Any]]]:
        """ChromaDBã§ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢"""
        results = {'text': [], 'images': []}

        try:
            where = {'category': category} if category else None

            if search_type in ['text', 'both']:
                text_results = self.text_collection.query(
                    query_embeddings=[query_embedding],
                    n_results=top_k,
                    where=where
                )
                results['text'] = self._format_chromadb_results(text_results)

            if search_type in ['image', 'both']:
                image_results = self.image_collection.query(
                    query_embeddings=[query_embedding],
                    n_results=top_k,
                    where=where
                )
                results['images'] = self._format_chromadb_results(image_results)

            logger.info(f"Search completed: {len(results['text'])} text, {len(results['images'])} images")

        except Exception as e:
            logger.error(f"Error during ChromaDB search: {e}")
            raise

        return results

    def _format_chromadb_results(self, raw_results: dict) -> List[Dict[str, Any]]:
        """ChromaDBæ¤œç´¢çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        formatted = []

        if not raw_results or not raw_results.get('ids'):
            return formatted

        for i in range(len(raw_results['ids'][0])):
            formatted.append({
                'id': raw_results['ids'][0][i],
                'document': raw_results['documents'][0][i],
                'metadata': raw_results['metadatas'][0][i],
                'distance': raw_results['distances'][0][i] if 'distances' in raw_results else None
            })

        return formatted

    def get_all_categories(self) -> List[str]:
        """
        ç™»éŒ²æ¸ˆã¿PDFã‹ã‚‰ä¸€æ„ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒªã‚¹ãƒˆã‚’å–å¾—

        Returns:
            list: ã‚«ãƒ†ã‚´ãƒªãƒ¼åã®ãƒªã‚¹ãƒˆï¼ˆé‡è¤‡ãªã—ã€ã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼‰
        """
        try:
            pdfs = self.get_registered_pdfs()
            categories = list(set(pdf['category'] for pdf in pdfs if pdf.get('category')))
            return sorted(categories)
        except Exception as e:
            logger.error(f"Error getting categories: {e}")
            return []

    def get_registered_pdfs(self) -> List[Dict[str, Any]]:
        """
        ç™»éŒ²æ¸ˆã¿PDFã®ãƒªã‚¹ãƒˆã‚’å–å¾—

        Returns:
            list: PDFãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®æƒ…å ±
        """
        if self.provider == 'supabase':
            return self._get_registered_pdfs_supabase()
        else:
            return self._get_registered_pdfs_chromadb()

    def _get_registered_pdfs_supabase(self) -> List[Dict[str, Any]]:
        """Supabaseã‹ã‚‰ç™»éŒ²æ¸ˆã¿PDFä¸€è¦§ã‚’å–å¾—"""
        try:
            # registered_pdfsãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å–å¾—
            response = self.client.table(self.pdf_table).select('*').execute()

            if not response.data:
                return []

            result = []
            for row in response.data:
                # ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®ä»¶æ•°ã‚’é›†è¨ˆ
                text_count = self.client.table(self.text_table)\
                    .select('id', count='exact')\
                    .eq('source_file', row['filename'])\
                    .execute()

                image_count = self.client.table(self.image_table)\
                    .select('id', count='exact')\
                    .eq('source_file', row['filename'])\
                    .execute()

                result.append({
                    'source_file': row['filename'],
                    'category': row['category'],
                    'text_count': text_count.count if text_count else 0,
                    'image_count': image_count.count if image_count else 0,
                    'total_count': (text_count.count if text_count else 0) + (image_count.count if image_count else 0)
                })

            logger.info(f"Found {len(result)} registered PDFs")
            return result

        except Exception as e:
            logger.error(f"Error getting registered PDFs from Supabase: {e}")
            return []

    def _get_registered_pdfs_chromadb(self) -> List[Dict[str, Any]]:
        """ChromaDBã‹ã‚‰ç™»éŒ²æ¸ˆã¿PDFä¸€è¦§ã‚’å–å¾—"""
        try:
            pdf_info = {}

            text_data = self.text_collection.get()
            if text_data and text_data.get('metadatas'):
                for metadata in text_data['metadatas']:
                    source_file = metadata.get('source_file', '')
                    if source_file:
                        if source_file not in pdf_info:
                            pdf_info[source_file] = {
                                'source_file': source_file,
                                'category': metadata.get('category', ''),
                                'text_count': 0,
                                'image_count': 0
                            }
                        pdf_info[source_file]['text_count'] += 1

            image_data = self.image_collection.get()
            if image_data and image_data.get('metadatas'):
                for metadata in image_data['metadatas']:
                    source_file = metadata.get('source_file', '')
                    if source_file:
                        if source_file not in pdf_info:
                            pdf_info[source_file] = {
                                'source_file': source_file,
                                'category': metadata.get('category', ''),
                                'text_count': 0,
                                'image_count': 0
                            }
                        pdf_info[source_file]['image_count'] += 1

            result = []
            for pdf_data in pdf_info.values():
                pdf_data['total_count'] = pdf_data['text_count'] + pdf_data['image_count']
                result.append(pdf_data)

            result.sort(key=lambda x: x['source_file'])

            logger.info(f"Found {len(result)} registered PDFs")
            return result

        except Exception as e:
            logger.error(f"Error getting registered PDFs from ChromaDB: {e}")
            return []

    def delete_by_source_file(self, source_file: str) -> Dict[str, int]:
        """
        ç‰¹å®šã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã«é–¢é€£ã™ã‚‹å…¨ã¦ã®ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤

        Args:
            source_file: å‰Šé™¤å¯¾è±¡ã®PDFãƒ•ã‚¡ã‚¤ãƒ«å

        Returns:
            dict: å‰Šé™¤ä»¶æ•° {'text_deleted': int, 'image_deleted': int}
        """
        if self.provider == 'supabase':
            return self._delete_by_source_file_supabase(source_file)
        else:
            return self._delete_by_source_file_chromadb(source_file)

    def _delete_by_source_file_supabase(self, source_file: str) -> Dict[str, int]:
        """Supabaseã‹ã‚‰ç‰¹å®šPDFã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤"""
        deleted_counts = {'text_deleted': 0, 'image_deleted': 0}

        try:
            # ãƒ†ã‚­ã‚¹ãƒˆå‰Šé™¤
            text_response = self.client.table(self.text_table)\
                .delete()\
                .eq('source_file', source_file)\
                .execute()
            deleted_counts['text_deleted'] = len(text_response.data) if text_response.data else 0

            # ç”»åƒå‰Šé™¤
            image_response = self.client.table(self.image_table)\
                .delete()\
                .eq('source_file', source_file)\
                .execute()
            deleted_counts['image_deleted'] = len(image_response.data) if image_response.data else 0

            # PDFç™»éŒ²æƒ…å ±å‰Šé™¤
            self.client.table(self.pdf_table)\
                .delete()\
                .eq('filename', source_file)\
                .execute()

            logger.info(f"Successfully deleted all data for {source_file} from Supabase")

        except Exception as e:
            logger.error(f"Error deleting data from Supabase for {source_file}: {e}")
            raise

        return deleted_counts

    def _delete_by_source_file_chromadb(self, source_file: str) -> Dict[str, int]:
        """ChromaDBã‹ã‚‰ç‰¹å®šPDFã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤"""
        deleted_counts = {'text_deleted': 0, 'image_deleted': 0}

        try:
            text_data = self.text_collection.get(where={'source_file': source_file})
            if text_data and text_data.get('ids'):
                text_ids = text_data['ids']
                if text_ids:
                    self.text_collection.delete(ids=text_ids)
                    deleted_counts['text_deleted'] = len(text_ids)

            image_data = self.image_collection.get(where={'source_file': source_file})
            if image_data and image_data.get('ids'):
                image_ids = image_data['ids']
                if image_ids:
                    self.image_collection.delete(ids=image_ids)
                    deleted_counts['image_deleted'] = len(image_ids)

            logger.info(f"Successfully deleted all data for {source_file} from ChromaDB")

        except Exception as e:
            logger.error(f"Error deleting data from ChromaDB for {source_file}: {e}")
            raise

        return deleted_counts

    def register_pdf(self, filename: str, category: str, storage_path: Optional[str] = None):
        """
        PDFã‚’registered_pdfsãƒ†ãƒ¼ãƒ–ãƒ«ã«ç™»éŒ²

        Args:
            filename: PDFãƒ•ã‚¡ã‚¤ãƒ«å
            category: ã‚«ãƒ†ã‚´ãƒªãƒ¼
            storage_path: Supabase Storageãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        """
        if self.provider == 'supabase':
            try:
                data = {
                    'filename': filename,
                    'category': category
                }
                if storage_path:
                    data['storage_path'] = storage_path

                self.client.table(self.pdf_table).upsert(data).execute()
                logger.info(f"Registered PDF in Supabase: {filename} (storage_path: {storage_path})")
            except Exception as e:
                logger.error(f"Error registering PDF in Supabase: {e}")
                raise

    def upload_pdf_to_storage(self, pdf_path: str, filename: str, category: str) -> str:
        """
        PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’Supabase Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰

        Args:
            pdf_path: ãƒ­ãƒ¼ã‚«ãƒ«ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            filename: PDFãƒ•ã‚¡ã‚¤ãƒ«å
            category: ã‚«ãƒ†ã‚´ãƒªãƒ¼

        Returns:
            str: Storageãƒ‘ã‚¹
        """
        if self.provider != 'supabase':
            logger.warning("PDF upload to storage is only supported for Supabase provider")
            return ""

        try:
            from pathlib import Path
            import hashlib
            from datetime import datetime

            # Storageãƒ‘ã‚¹ã‚’ç”Ÿæˆï¼ˆæ—¥æœ¬èªã‚’é¿ã‘ã‚‹ãŸã‚ã€ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¹ã‚’ä½¿ç”¨ï¼‰
            # ã‚«ãƒ†ã‚´ãƒªãƒ¼ã¨æ—¥æ™‚ã®ãƒãƒƒã‚·ãƒ¥ã§ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            category_hash = hashlib.md5(category.encode('utf-8')).hexdigest()[:8]
            timestamp = datetime.now().strftime("%Y%m%d")

            # ãƒ•ã‚¡ã‚¤ãƒ«åã®æ‹¡å¼µå­ã‚’ä¿æŒ
            file_extension = Path(filename).suffix
            filename_hash = hashlib.md5(filename.encode('utf-8')).hexdigest()[:16]

            # è‹±æ•°å­—ã®ã¿ã®ãƒ‘ã‚¹ã‚’ç”Ÿæˆ: cat_{hash}/file_{hash}_{timestamp}.pdf
            storage_path = f"cat_{category_hash}/file_{filename_hash}_{timestamp}{file_extension}"

            # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            with open(pdf_path, 'rb') as f:
                pdf_bytes = f.read()

            # Supabase Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            self.client.storage.from_(self.pdf_storage_bucket).upload(
                storage_path,
                pdf_bytes,
                file_options={"content-type": "application/pdf", "upsert": "true"}
            )
            logger.info(f"Uploaded PDF to Supabase Storage: {storage_path}")

            return storage_path

        except Exception as e:
            logger.error(f"Error uploading PDF to Supabase Storage: {e}")
            raise

    def get_pdf_url_from_storage(self, filename: str) -> Optional[str]:
        """
        Supabase Storageã‹ã‚‰PDFã®ç½²åä»˜ãURLã‚’å–å¾—

        Args:
            filename: PDFãƒ•ã‚¡ã‚¤ãƒ«å

        Returns:
            str: ç½²åä»˜ãURLï¼ˆæœ‰åŠ¹æœŸé™: 1æ™‚é–“ï¼‰ã€å–å¾—å¤±æ•—æ™‚ã¯None
        """
        if self.provider != 'supabase':
            return None

        try:
            # registered_pdfsãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰storage_pathã‚’å–å¾—
            response = self.client.table(self.pdf_table)\
                .select('storage_path')\
                .eq('filename', filename)\
                .execute()

            if not response.data or len(response.data) == 0:
                logger.warning(f"PDF not found in database: {filename}")
                return None

            storage_path = response.data[0].get('storage_path')
            if not storage_path:
                logger.warning(f"No storage_path found for PDF: {filename}")
                return None

            # ç½²åä»˜ãURLã‚’ç”Ÿæˆï¼ˆæœ‰åŠ¹æœŸé™: 3600ç§’ = 1æ™‚é–“ï¼‰
            url_response = self.client.storage.from_(self.pdf_storage_bucket)\
                .create_signed_url(storage_path, 3600)

            if url_response and 'signedURL' in url_response:
                logger.info(f"Generated signed URL for PDF: {filename}")
                return url_response['signedURL']
            else:
                logger.error(f"Failed to generate signed URL for PDF: {filename}")
                return None

        except Exception as e:
            logger.error(f"Error getting PDF URL from Supabase Storage: {e}")
            return None

    def download_pdf_from_storage(self, filename: str, destination_path: str) -> bool:
        """
        Supabase Storageã‹ã‚‰PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

        Args:
            filename: PDFãƒ•ã‚¡ã‚¤ãƒ«å
            destination_path: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆãƒ‘ã‚¹

        Returns:
            bool: æˆåŠŸæ™‚Trueã€å¤±æ•—æ™‚False
        """
        if self.provider != 'supabase':
            return False

        try:
            # registered_pdfsãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰storage_pathã‚’å–å¾—
            response = self.client.table(self.pdf_table)\
                .select('storage_path')\
                .eq('filename', filename)\
                .execute()

            if not response.data or len(response.data) == 0:
                logger.warning(f"PDF not found in database: {filename}")
                return False

            storage_path = response.data[0].get('storage_path')
            if not storage_path:
                logger.warning(f"No storage_path found for PDF: {filename}")
                return False

            # PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            pdf_bytes = self.client.storage.from_(self.pdf_storage_bucket)\
                .download(storage_path)

            # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with open(destination_path, 'wb') as f:
                f.write(pdf_bytes)

            logger.info(f"Downloaded PDF from Supabase Storage: {filename} -> {destination_path}")
            return True

        except Exception as e:
            logger.error(f"Error downloading PDF from Supabase Storage: {e}")
            return False

    def debug_inspect_data(self, category: str, limit: int = 5) -> Dict[str, Any]:
        """
        ãƒ‡ãƒãƒƒã‚°ç”¨: ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª

        Args:
            category: ã‚«ãƒ†ã‚´ãƒªãƒ¼å
            limit: å–å¾—ã™ã‚‹ä»¶æ•°

        Returns:
            dict: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
        """
        if self.provider != 'supabase':
            logger.warning("debug_inspect_data is only supported for Supabase")
            return {}

        try:
            result = {
                'category': category,
                'text_chunks': [],
                'images': []
            }

            # ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®ã‚µãƒ³ãƒ—ãƒ«å–å¾—
            text_response = self.client.table(self.text_table)\
                .select('id, content, source_file, page_number, category')\
                .eq('category', category)\
                .limit(limit)\
                .execute()

            if text_response.data:
                result['text_chunks'] = text_response.data
                logger.info(f"ğŸ“Š DEBUG: Sample text chunks for '{category}':")
                for i, chunk in enumerate(text_response.data[:3], 1):
                    logger.info(f"  [{i}] {chunk['source_file']} (page {chunk['page_number']})")
                    logger.info(f"      Content preview: {chunk['content'][:100]}...")

            # ç”»åƒã®ã‚µãƒ³ãƒ—ãƒ«å–å¾—
            image_response = self.client.table(self.image_table)\
                .select('id, content, source_file, page_number, category, content_type')\
                .eq('category', category)\
                .limit(limit)\
                .execute()

            if image_response.data:
                result['images'] = image_response.data
                logger.info(f"ğŸ“Š DEBUG: Sample images for '{category}':")
                for i, img in enumerate(image_response.data[:3], 1):
                    logger.info(f"  [{i}] {img['source_file']} (page {img['page_number']}, type: {img.get('content_type', 'image')})")
                    logger.info(f"      Description preview: {img['content'][:100]}...")

            # embeddingãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            text_with_emb = self.client.table(self.text_table)\
                .select('id, embedding')\
                .eq('category', category)\
                .limit(1)\
                .execute()

            if text_with_emb.data and len(text_with_emb.data) > 0:
                embedding = text_with_emb.data[0].get('embedding')
                if embedding:
                    logger.info(f"âœ… DEBUG: Embedding exists, dimension: {len(embedding)}")
                    logger.info(f"ğŸ” DEBUG: Embedding type: {type(embedding)}")
                    logger.info(f"ğŸ” DEBUG: First 5 elements: {embedding[:5]}")

                    # ç•°å¸¸ãªæ¬¡å…ƒæ•°ã®å ´åˆã¯è­¦å‘Š
                    if len(embedding) != 3072:
                        logger.error(f"âŒ DEBUG: ABNORMAL embedding dimension! Expected 3072, got {len(embedding)}")
                        logger.error(f"   This will cause vector search to fail!")
                else:
                    logger.error(f"âŒ DEBUG: Embedding field is NULL!")

            return result

        except Exception as e:
            logger.error(f"Error inspecting data: {e}")
            return {}
