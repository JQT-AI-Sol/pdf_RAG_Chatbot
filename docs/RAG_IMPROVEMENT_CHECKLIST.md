# RAGæ€§èƒ½å‘ä¸Š Phase 1 å®Ÿè£…ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

**å¯¾è±¡ãƒ•ã‚§ãƒ¼ã‚º**: Phase 1 - Quick Winsï¼ˆ1é€±é–“ï¼‰
**é–‹å§‹æ—¥**: ___________
**å®Œäº†äºˆå®šæ—¥**: ___________

---

## ğŸ“‹ å®Ÿè£…æ¦‚è¦

| ã‚¿ã‚¹ã‚¯ | å„ªå…ˆåº¦ | æ¨å®šæ™‚é–“ | çŠ¶æ…‹ |
|--------|--------|----------|------|
| 1. Rerankingå°å…¥ | ğŸ”´ æœ€é«˜ | 1-2æ—¥ | â¬œ æœªç€æ‰‹ |
| 2. ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚° | ğŸŸ  é«˜ | 1æ—¥ | â¬œ æœªç€æ‰‹ |
| 3. ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚° | ğŸŸ¡ ä¸­ | 1æ—¥ | â¬œ æœªç€æ‰‹ |
| 4. Visionè§£æã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚° | ğŸŸ¡ ä¸­ | 0.5æ—¥ | â¬œ æœªç€æ‰‹ |
| 5. BM25ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ | ğŸŸ¢ æ¨å¥¨ | 1-2æ—¥ | â¬œ æœªç€æ‰‹ |
| 6. çµ±åˆãƒ†ã‚¹ãƒˆ & ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ | ğŸ”µ å¿…é ˆ | 1æ—¥ | â¬œ æœªç€æ‰‹ |

**çŠ¶æ…‹è¨˜å·**: â¬œ æœªç€æ‰‹ / ğŸŸ¡ é€²è¡Œä¸­ / âœ… å®Œäº† / âŒ ãƒ–ãƒ­ãƒƒã‚¯

---

## ğŸ¯ ã‚¿ã‚¹ã‚¯ 1: Rerankingå°å…¥

### æº–å‚™

- [ ] ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
  ```bash
  cd backend
  pip install sentence-transformers==2.2.2
  ```

- [ ] ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã ãŒæ¨å¥¨ï¼‰
  ```bash
  python -c "from sentence_transformers import CrossEncoder; CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
  ```
  - æ¨å®šæ™‚é–“: 2-5åˆ†
  - ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: ~80MB

### å®Ÿè£…

- [ ] æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: `backend/src/reranker.py`
  - [ ] `Reranker`ã‚¯ãƒ©ã‚¹ã‚’å®Ÿè£…
  - [ ] `__init__()`: ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
  - [ ] `rerank()`: ã‚¯ã‚¨ãƒªã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®rerankingãƒ­ã‚¸ãƒƒã‚¯

<details>
<summary>ğŸ“ å®Ÿè£…ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ</summary>

```python
# backend/src/reranker.py

from sentence_transformers import CrossEncoder
from typing import List, Tuple
import numpy as np
import logging

logger = logging.getLogger(__name__)

class Reranker:
    def __init__(self, model_name: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2'):
        """
        Cross-Encoderãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–

        Args:
            model_name: ä½¿ç”¨ã™ã‚‹Cross-Encoderãƒ¢ãƒ‡ãƒ«å
        """
        logger.info(f"Initializing Reranker with model: {model_name}")
        self.model = CrossEncoder(model_name)
        logger.info("Reranker initialized successfully")

    def rerank(
        self,
        query: str,
        documents: List[str],
        top_k: int = 5
    ) -> List[Tuple[int, float]]:
        """
        ã‚¯ã‚¨ãƒªã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒšã‚¢ã‚’rerankã—ã€
        ã‚¹ã‚³ã‚¢ã®é«˜ã„é †ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã‚¹ã‚³ã‚¢ã‚’è¿”ã™

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            documents: rerankã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
            top_k: è¿”å´ã™ã‚‹ä¸Šä½Kä»¶

        Returns:
            (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹, ã‚¹ã‚³ã‚¢)ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        if not documents:
            return []

        # ã‚¯ã‚¨ãƒª-ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒšã‚¢ã‚’ä½œæˆ
        pairs = [[query, doc] for doc in documents]

        # rerankã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        scores = self.model.predict(pairs)

        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        ranked_indices = np.argsort(scores)[::-1][:top_k]
        ranked_scores = scores[ranked_indices]

        logger.debug(
            f"Reranking completed: {len(documents)} docs -> top {top_k}",
            extra={"scores": ranked_scores.tolist()}
        )

        return list(zip(ranked_indices.tolist(), ranked_scores.tolist()))
```
</details>

- [ ] `backend/src/rag_engine.py` ã‚’ä¿®æ­£
  - [ ] `Reranker`ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
  - [ ] `__init__()`: Rerankerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–
  - [ ] `_get_relevant_contexts()`: Rerankingãƒ­ã‚¸ãƒƒã‚¯ã‚’çµ±åˆ

<details>
<summary>ğŸ“ çµ±åˆã‚³ãƒ¼ãƒ‰ä¾‹</summary>

```python
# backend/src/rag_engine.py ã®ä¿®æ­£ç®‡æ‰€

from src.reranker import Reranker

class RAGEngine:
    def __init__(self, config):
        # ...æ—¢å­˜ã‚³ãƒ¼ãƒ‰...

        # RerankeråˆæœŸåŒ–
        if config["rag"].get("enable_reranking", True):
            reranking_model = config.get("reranking", {}).get(
                "model",
                "cross-encoder/ms-marco-MiniLM-L-6-v2"
            )
            self.reranker = Reranker(model_name=reranking_model)
        else:
            self.reranker = None

    def _get_relevant_contexts(
        self,
        query: str,
        category: Optional[str] = None
    ) -> List[Dict]:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ï¼ˆRerankingçµ±åˆç‰ˆï¼‰"""
        query_embedding = self.embedder.embed_text(query)

        # 1æ¬¡æ¤œç´¢: å¤šã‚ã«å–å¾—
        top_k_initial = self.config.get("reranking", {}).get("top_k_initial", 10)

        text_results = self.vector_store.similarity_search(
            query_embedding,
            k=top_k_initial,
            filter={"category": category} if category else None
        )

        # Rerankingã‚’é©ç”¨
        if self.reranker and text_results:
            documents = [r.page_content for r in text_results]
            top_k_final = self.config.get("reranking", {}).get("top_k_final", 5)

            reranked_indices, scores = self.reranker.rerank(
                query,
                documents,
                top_k=top_k_final
            )

            # ä¸Šä½Kä»¶ã‚’é¸æŠ
            text_results = [text_results[idx] for idx, _ in reranked_indices]

        return text_results
```
</details>

- [ ] `config.yaml` ã«è¨­å®šã‚’è¿½åŠ 
  ```yaml
  rag:
    enable_reranking: true

  reranking:
    model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
    top_k_initial: 10
    top_k_final: 5
  ```

### ãƒ†ã‚¹ãƒˆ

- [ ] ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’ä½œæˆ: `backend/tests/test_reranker.py`
  - [ ] `test_reranker_initialization`: ãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ãåˆæœŸåŒ–ã•ã‚Œã‚‹ã‹
  - [ ] `test_reranker_improves_ranking`: é–¢é€£æ€§ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒä¸Šä½ã«æ¥ã‚‹ã‹
  - [ ] `test_reranker_empty_documents`: ç©ºã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆã®å‡¦ç†

<details>
<summary>ğŸ“ ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ä¾‹</summary>

```python
# backend/tests/test_reranker.py

import pytest
from src.reranker import Reranker

class TestReranker:
    @pytest.fixture
    def reranker(self):
        return Reranker()

    def test_reranker_initialization(self, reranker):
        """RerankerãŒæ­£ã—ãåˆæœŸåŒ–ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª"""
        assert reranker.model is not None

    def test_reranker_improves_ranking(self, reranker):
        """Rerankingã§é–¢é€£æ€§ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒä¸Šä½ã«æ¥ã‚‹ã“ã¨ã‚’ç¢ºèª"""
        query = "è¡¨ã®ä½œæˆæ–¹æ³•"
        documents = [
            "ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã™ã‚‹ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠã—ã¦ã‚°ãƒ©ãƒ•ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ä½œæˆã—ã¾ã™ã€‚",
            "è¡¨ã‚’ä½œæˆã™ã‚‹ã«ã¯ã€æŒ¿å…¥ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰è¡¨ã‚’é¸æŠã—ã€è¡Œæ•°ã¨åˆ—æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚",
            "å›³ã‚’æŒ¿å…¥ã™ã‚‹ã«ã¯ã€æŒ¿å…¥ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰å›³ã‚’é¸æŠã—ã¾ã™ã€‚"
        ]

        reranked_indices, scores = reranker.rerank(query, documents, top_k=1)

        # 2ç•ªç›®ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹1ï¼‰ãŒæœ€ã‚‚é–¢é€£æ€§ãŒé«˜ã„
        assert reranked_indices[0] == 1
        assert scores[0] > 0.5  # ã‚¹ã‚³ã‚¢ãŒååˆ†é«˜ã„ã“ã¨ã‚’ç¢ºèª

    def test_reranker_empty_documents(self, reranker):
        """ç©ºã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆã§ã‚‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãªã„ã“ã¨ã‚’ç¢ºèª"""
        query = "ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª"
        documents = []

        result = reranker.rerank(query, documents)

        assert result == []
```
</details>

- [ ] çµ±åˆãƒ†ã‚¹ãƒˆ
  ```bash
  cd backend
  pytest tests/test_reranker.py -v
  ```

- [ ] æ‰‹å‹•ãƒ†ã‚¹ãƒˆ: ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã§æ¤œç´¢ç²¾åº¦ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’ç¢ºèª

### å®Œäº†æ¡ä»¶

- [ ] ãƒ†ã‚¹ãƒˆãŒã™ã¹ã¦ãƒ‘ã‚¹ã™ã‚‹
- [ ] RerankingãŒæœ‰åŠ¹ãªå ´åˆã€æ¤œç´¢çµæœã®é †ä½ãŒå¤‰ã‚ã‚‹
- [ ] Rerankingã‚’ç„¡åŠ¹ã«ã—ã¦ã‚‚æ—¢å­˜æ©Ÿèƒ½ãŒå‹•ä½œã™ã‚‹
- [ ] ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å¢—åŠ ãŒ100msä»¥å†…

---

## ğŸ¯ ã‚¿ã‚¹ã‚¯ 2: ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°

### æº–å‚™

- [ ] LangChainã®`RecursiveCharacterTextSplitter`ã‚’ç¢ºèª
  ```bash
  python -c "from langchain.text_splitter import RecursiveCharacterTextSplitter; print('OK')"
  ```

### å®Ÿè£…

- [ ] `backend/src/pdf_processor.py` ã‚’ä¿®æ­£
  - [ ] `RecursiveCharacterTextSplitter`ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
  - [ ] `__init__()`: text_splitterã‚’åˆæœŸåŒ–
  - [ ] `_chunk_text()`: ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã‚’å®Ÿè£…
  - [ ] `_preserve_table_context()`: è¡¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¿æŒãƒ­ã‚¸ãƒƒã‚¯ï¼ˆæ–°è¦ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰

<details>
<summary>ğŸ“ å®Ÿè£…ã‚³ãƒ¼ãƒ‰ä¾‹</summary>

```python
# backend/src/pdf_processor.py ã®ä¿®æ­£ç®‡æ‰€

from langchain.text_splitter import RecursiveCharacterTextSplitter
import re

class PDFProcessor:
    def __init__(self, config):
        # ...æ—¢å­˜ã‚³ãƒ¼ãƒ‰...

        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚«ãƒ¼ã‚’åˆæœŸåŒ–
        if config["rag"].get("enable_semantic_chunking", True):
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=config["chunking"]["chunk_size"],
                chunk_overlap=config["chunking"]["chunk_overlap"],
                length_function=self._count_tokens,
                separators=config["chunking"]["separators"],
                keep_separator=True
            )
        else:
            self.text_splitter = None

    def _chunk_text(self, text: str, page_num: int) -> List[Dict]:
        """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯å¢ƒç•Œã‚’è€ƒæ…®ã—ãŸãƒãƒ£ãƒ³ã‚¯åŒ–"""

        # è¡¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿æŒ
        text = self._preserve_table_context(text)

        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°
        if self.text_splitter:
            chunks = self.text_splitter.split_text(text)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—¢å­˜ã®ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°
            chunks = self._legacy_chunk_text(text)

        return [
            {
                "content": chunk,
                "page_number": page_num,
                "content_type": "text",
                "chunk_index": idx
            }
            for idx, chunk in enumerate(chunks)
        ]

    def _preserve_table_context(self, text: str) -> str:
        """
        è¡¨ã®å‰å¾Œã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’ä¿æŒ
        è¡¨ã‚„å›³ã®å‚ç…§ã‚’æ¤œå‡ºã—ã€æ®µè½åŒºåˆ‡ã‚Šã‚’å¼·åŒ–
        """
        patterns = [
            r'(è¡¨\s*\d+[.:].*?)(\n)',
            r'(å›³\s*\d+[.:].*?)(\n)',
            r'(Table\s+\d+[.:].*?)(\n)',
            r'(Figure\s+\d+[.:].*?)(\n)',
        ]

        for pattern in patterns:
            text = re.sub(pattern, r'\1\n\n', text)

        return text

    def _legacy_chunk_text(self, text: str) -> List[str]:
        """æ—¢å­˜ã®ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰"""
        # ...æ—¢å­˜ã®å®Ÿè£…...
        pass
```
</details>

- [ ] `config.yaml` ã«è¨­å®šã‚’è¿½åŠ 
  ```yaml
  rag:
    enable_semantic_chunking: true

  chunking:
    chunk_size: 800
    chunk_overlap: 150
    separators:
      - "\n\n"
      - "\n"
      - "ã€‚"
      - "ï¼"
      - ". "
      - "! "
      - "? "
      - "ï¼›"
      - "ã€"
      - ", "
      - " "
      - ""
  ```

### ãƒ†ã‚¹ãƒˆ

- [ ] ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’ä½œæˆ/æ›´æ–°: `backend/tests/test_pdf_processor.py`
  - [ ] `test_semantic_chunking_preserves_paragraphs`: æ®µè½ãŒä¿æŒã•ã‚Œã‚‹ã‹
  - [ ] `test_table_context_preservation`: è¡¨ã®è¦‹å‡ºã—ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹ã‹
  - [ ] `test_chunking_respects_semantic_boundaries`: ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯å¢ƒç•Œã§åˆ†å‰²ã•ã‚Œã‚‹ã‹

<details>
<summary>ğŸ“ ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ä¾‹</summary>

```python
def test_table_context_preservation(processor):
    """è¡¨ã®è¦‹å‡ºã—ãŒè¡¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨åŒã˜ãƒãƒ£ãƒ³ã‚¯ã«å«ã¾ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª"""
    text = """
    ã“ã‚Œã¯å‰ã®ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚

    è¡¨1: ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±
    åå‰ | å¹´é½¢ | ä½æ‰€
    ç”°ä¸­ | 30 | æ±äº¬
    éˆ´æœ¨ | 25 | å¤§é˜ª

    ã“ã‚Œã¯å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚
    """

    chunks = processor._chunk_text(text, page_num=1)

    # è¡¨ã®è¦‹å‡ºã—ã¨å†…å®¹ãŒåŒã˜ãƒãƒ£ãƒ³ã‚¯ã«å«ã¾ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
    table_chunks = [c for c in chunks if "è¡¨1" in c["content"]]
    assert len(table_chunks) > 0
    assert "ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±" in table_chunks[0]["content"]
    assert "ç”°ä¸­" in table_chunks[0]["content"]
```
</details>

- [ ] çµ±åˆãƒ†ã‚¹ãƒˆ
  ```bash
  pytest tests/test_pdf_processor.py::test_table_context_preservation -v
  ```

- [ ] æ‰‹å‹•ãƒ†ã‚¹ãƒˆ: è¡¨ã‚’å«ã‚€PDFã§ã€è¡¨ã®è¦‹å‡ºã—ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒåŒã˜ãƒãƒ£ãƒ³ã‚¯ã«ãªã‚‹ã“ã¨ã‚’ç¢ºèª

### å®Œäº†æ¡ä»¶

- [ ] ãƒ†ã‚¹ãƒˆãŒã™ã¹ã¦ãƒ‘ã‚¹ã™ã‚‹
- [ ] è¡¨ã®è¦‹å‡ºã—ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒåŒã˜ãƒãƒ£ãƒ³ã‚¯ã«å«ã¾ã‚Œã‚‹
- [ ] ãƒãƒ£ãƒ³ã‚¯å¢ƒç•ŒãŒæ–‡ã®é€”ä¸­ã§ãªã„ï¼ˆæ—¥æœ¬èªãƒ»è‹±èªä¸¡æ–¹ï¼‰
- [ ] æ—¢å­˜ã®ãƒãƒ£ãƒ³ã‚¯æ•°ã¨å¤§ããå¤‰ã‚ã‚‰ãªã„ï¼ˆÂ±20%ä»¥å†…ï¼‰

---

## ğŸ¯ ã‚¿ã‚¹ã‚¯ 3: ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°

### æº–å‚™

- [ ] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
  ```bash
  mkdir backend\cache\embeddings
  ```

### å®Ÿè£…

- [ ] æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: `backend/src/embedding_cache.py`
  - [ ] `EmbeddingCache`ã‚¯ãƒ©ã‚¹ã‚’å®Ÿè£…
  - [ ] `get()`: ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—
  - [ ] `set()`: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
  - [ ] `_get_cache_key()`: ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
  - [ ] ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆLRUï¼‰ã¨ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ä¸¡æ–¹ã‚’å®Ÿè£…

<details>
<summary>ğŸ“ å®Ÿè£…ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ</summary>

```python
# backend/src/embedding_cache.py

import hashlib
import pickle
from typing import List, Optional
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class EmbeddingCache:
    def __init__(self, cache_dir: str = "./cache/embeddings", max_memory_items: int = 1000):
        """
        ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆæœŸåŒ–

        Args:
            cache_dir: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
            max_memory_items: ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ€å¤§ã‚¢ã‚¤ãƒ†ãƒ æ•°
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self._memory_cache = {}
        self._max_memory_items = max_memory_items
        logger.info(f"EmbeddingCache initialized: {cache_dir}")

    def _get_cache_key(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ï¼ˆSHA256ãƒãƒƒã‚·ãƒ¥ï¼‰ã‚’ç”Ÿæˆ"""
        return hashlib.sha256(text.encode('utf-8')).hexdigest()

    def get(self, text: str) -> Optional[List[float]]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—"""
        key = self._get_cache_key(text)

        # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
        if key in self._memory_cache:
            logger.debug(f"Memory cache hit: {key[:8]}...")
            return self._memory_cache[key]

        # ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
        cache_file = self.cache_dir / f"{key}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    embedding = pickle.load(f)
                logger.debug(f"Disk cache hit: {key[:8]}...")

                # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«è¿½åŠ 
                self._add_to_memory_cache(key, embedding)
                return embedding
            except Exception as e:
                logger.warning(f"Failed to load cache: {e}")

        logger.debug(f"Cache miss: {key[:8]}...")
        return None

    def set(self, text: str, embedding: List[float]):
        """åŸ‹ã‚è¾¼ã¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        key = self._get_cache_key(text)

        # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«è¿½åŠ 
        self._add_to_memory_cache(key, embedding)

        # ãƒ‡ã‚£ã‚¹ã‚¯ã«æ°¸ç¶šåŒ–
        cache_file = self.cache_dir / f"{key}.pkl"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(embedding, f)
            logger.debug(f"Cached embedding: {key[:8]}...")
        except Exception as e:
            logger.warning(f"Failed to save cache: {e}")

    def _add_to_memory_cache(self, key: str, value: List[float]):
        """LRUæ–¹å¼ã§ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«è¿½åŠ """
        if len(self._memory_cache) >= self._max_memory_items:
            # æœ€ã‚‚å¤ã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’å‰Šé™¤
            oldest_key = next(iter(self._memory_cache))
            del self._memory_cache[oldest_key]

        self._memory_cache[key] = value
```
</details>

- [ ] `backend/src/text_embedder.py` ã‚’ä¿®æ­£
  - [ ] `EmbeddingCache`ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
  - [ ] `__init__()`: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆæœŸåŒ–
  - [ ] `embed_text()`: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ã‚’è¿½åŠ 
  - [ ] `embed_batch()`: ãƒãƒƒãƒå‡¦ç†ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ´»ç”¨

<details>
<summary>ğŸ“ çµ±åˆã‚³ãƒ¼ãƒ‰ä¾‹</summary>

```python
# backend/src/text_embedder.py ã®ä¿®æ­£ç®‡æ‰€

from src.embedding_cache import EmbeddingCache

class TextEmbedder:
    def __init__(self, config):
        # ...æ—¢å­˜ã‚³ãƒ¼ãƒ‰...

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆæœŸåŒ–
        if config["cache"]["embedding"].get("enabled", True):
            cache_dir = config["cache"]["embedding"].get("directory", "./cache/embeddings")
            max_items = config["cache"]["embedding"].get("max_memory_items", 1000)
            self.cache = EmbeddingCache(cache_dir=cache_dir, max_memory_items=max_items)
        else:
            self.cache = None

    def embed_text(self, text: str) -> List[float]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
        if self.cache:
            cached = self.cache.get(text)
            if cached is not None:
                return cached

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹: APIå‘¼ã³å‡ºã—
        embedding = self._call_openai_api(text)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        if self.cache:
            self.cache.set(text, embedding)

        return embedding

    def embed_batch(self, texts: List[str]) -> List[List[float]]:
        """ãƒãƒƒãƒå‡¦ç†ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ´»ç”¨"""
        results = [None] * len(texts)
        uncached_texts = []
        uncached_indices = []

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ/ãƒŸã‚¹ã‚’åˆ¤å®š
        if self.cache:
            for idx, text in enumerate(texts):
                cached = self.cache.get(text)
                if cached is not None:
                    results[idx] = cached
                else:
                    uncached_texts.append(text)
                    uncached_indices.append(idx)
        else:
            uncached_texts = texts
            uncached_indices = list(range(len(texts)))

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒƒãƒå‡¦ç†
        if uncached_texts:
            embeddings = self._call_openai_api_batch(uncached_texts)

            # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ & resultsé…åˆ—ã«æ ¼ç´
            for idx, text, embedding in zip(uncached_indices, uncached_texts, embeddings):
                if self.cache:
                    self.cache.set(text, embedding)
                results[idx] = embedding

        return results
```
</details>

- [ ] `config.yaml` ã«è¨­å®šã‚’è¿½åŠ 
  ```yaml
  cache:
    embedding:
      enabled: true
      directory: "./cache/embeddings"
      max_memory_items: 1000
  ```

### ãƒ†ã‚¹ãƒˆ

- [ ] ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’ä½œæˆ: `backend/tests/test_embedding_cache.py`
  - [ ] `test_cache_hit`: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã®å‹•ä½œ
  - [ ] `test_cache_miss`: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ã®å‹•ä½œ
  - [ ] `test_cache_persistence`: ãƒ‡ã‚£ã‚¹ã‚¯ã¸ã®æ°¸ç¶šåŒ–

<details>
<summary>ğŸ“ ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ä¾‹</summary>

```python
# backend/tests/test_embedding_cache.py

import pytest
from src.embedding_cache import EmbeddingCache
import tempfile
import shutil

class TestEmbeddingCache:
    @pytest.fixture
    def cache_dir(self):
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        temp_dir = tempfile.mkdtemp()
        yield temp_dir
        # ãƒ†ã‚¹ãƒˆå¾Œã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        shutil.rmtree(temp_dir)

    @pytest.fixture
    def cache(self, cache_dir):
        return EmbeddingCache(cache_dir=cache_dir, max_memory_items=10)

    def test_cache_miss(self, cache):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãªã„ãƒ†ã‚­ã‚¹ãƒˆã¯NoneãŒè¿”ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª"""
        result = cache.get("å­˜åœ¨ã—ãªã„ãƒ†ã‚­ã‚¹ãƒˆ")
        assert result is None

    def test_cache_hit(self, cache):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã—ãŸãƒ†ã‚­ã‚¹ãƒˆãŒå–å¾—ã§ãã‚‹ã“ã¨ã‚’ç¢ºèª"""
        text = "ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ"
        embedding = [0.1, 0.2, 0.3]

        cache.set(text, embedding)
        result = cache.get(text)

        assert result == embedding

    def test_cache_persistence(self, cache_dir):
        """ãƒ‡ã‚£ã‚¹ã‚¯ã«æ°¸ç¶šåŒ–ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª"""
        cache1 = EmbeddingCache(cache_dir=cache_dir)
        text = "æ°¸ç¶šåŒ–ãƒ†ã‚¹ãƒˆ"
        embedding = [0.5, 0.6, 0.7]

        cache1.set(text, embedding)

        # æ–°ã—ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
        cache2 = EmbeddingCache(cache_dir=cache_dir)
        result = cache2.get(text)

        assert result == embedding
```
</details>

- [ ] çµ±åˆãƒ†ã‚¹ãƒˆ
  ```bash
  pytest tests/test_embedding_cache.py -v
  ```

- [ ] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã®æ¸¬å®š
  ```python
  # backend/evaluation/measure_cache_hit_rate.py
  # åŒã˜ã‚¯ã‚¨ãƒªã‚’è¤‡æ•°å›å®Ÿè¡Œã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã‚’æ¸¬å®š
  ```

### å®Œäº†æ¡ä»¶

- [ ] ãƒ†ã‚¹ãƒˆãŒã™ã¹ã¦ãƒ‘ã‚¹ã™ã‚‹
- [ ] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ™‚ã€APIå‘¼ã³å‡ºã—ãŒç™ºç”Ÿã—ãªã„
- [ ] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ãŒ60%ä»¥ä¸Šï¼ˆå†å‡¦ç†æ™‚ï¼‰
- [ ] ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæ°¸ç¶šåŒ–ã•ã‚Œã‚‹

---

## ğŸ¯ ã‚¿ã‚¹ã‚¯ 4: Visionè§£æã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°

### æº–å‚™

- [ ] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
  ```bash
  mkdir backend\cache\vision_analysis
  ```

### å®Ÿè£…

- [ ] `backend/src/vision_analyzer.py` ã‚’ä¿®æ­£
  - [ ] `__init__()`: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š
  - [ ] `_get_image_hash()`: ç”»åƒãƒãƒƒã‚·ãƒ¥ç”Ÿæˆï¼ˆæ–°è¦ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
  - [ ] `analyze_image()`: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ã‚’è¿½åŠ 

<details>
<summary>ğŸ“ å®Ÿè£…ã‚³ãƒ¼ãƒ‰ä¾‹</summary>

```python
# backend/src/vision_analyzer.py ã®ä¿®æ­£ç®‡æ‰€

import hashlib
import json
from pathlib import Path
from datetime import datetime

class VisionAnalyzer:
    def __init__(self, config):
        # ...æ—¢å­˜ã‚³ãƒ¼ãƒ‰...

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        if config["cache"]["vision"].get("enabled", True):
            cache_dir = config["cache"]["vision"].get("directory", "./cache/vision_analysis")
            self.cache_dir = Path(cache_dir)
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        else:
            self.cache_dir = None

    def _get_image_hash(self, image_path: str) -> str:
        """ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®SHA256ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—"""
        with open(image_path, 'rb') as f:
            return hashlib.sha256(f.read()).hexdigest()

    def analyze_image(
        self,
        image_path: str,
        analysis_type: str
    ) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãç”»åƒè§£æ"""

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if self.cache_dir:
            image_hash = self._get_image_hash(image_path)
            cache_key = f"{image_hash}_{analysis_type}"
            cache_file = self.cache_dir / f"{cache_key}.json"

            if cache_file.exists():
                try:
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        cached_data = json.load(f)
                    logger.debug(f"Vision cache hit: {cache_key[:8]}...")
                    return cached_data['result']
                except Exception as e:
                    logger.warning(f"Failed to load vision cache: {e}")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹: Gemini APIå‘¼ã³å‡ºã—
        result = self._call_gemini_vision(image_path, analysis_type)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        if self.cache_dir:
            try:
                with open(cache_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        'result': result,
                        'timestamp': datetime.now().isoformat(),
                        'analysis_type': analysis_type,
                        'image_hash': image_hash
                    }, f, ensure_ascii=False, indent=2)
                logger.debug(f"Cached vision result: {cache_key[:8]}...")
            except Exception as e:
                logger.warning(f"Failed to save vision cache: {e}")

        return result
```
</details>

- [ ] `config.yaml` ã«è¨­å®šã‚’è¿½åŠ 
  ```yaml
  cache:
    vision:
      enabled: true
      directory: "./cache/vision_analysis"
      expiry_days: 30  # ã‚ªãƒ—ã‚·ãƒ§ãƒ³
  ```

### ãƒ†ã‚¹ãƒˆ

- [ ] ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’ä½œæˆ/æ›´æ–°: `backend/tests/test_vision_analyzer.py`
  - [ ] `test_vision_cache_hit`: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã®å‹•ä½œ
  - [ ] `test_vision_cache_miss`: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ã®å‹•ä½œ
  - [ ] `test_different_analysis_types`: è§£æã‚¿ã‚¤ãƒ—ã”ã¨ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒåˆ†ã‹ã‚Œã‚‹ã‹

- [ ] çµ±åˆãƒ†ã‚¹ãƒˆ
  ```bash
  pytest tests/test_vision_analyzer.py -v
  ```

- [ ] æ‰‹å‹•ãƒ†ã‚¹ãƒˆ: åŒã˜PDFã‚’2å›å‡¦ç†ã—ã€2å›ç›®ã§Vision APIå‘¼ã³å‡ºã—ãŒæ¸›ã‚‹ã“ã¨ã‚’ç¢ºèª

### å®Œäº†æ¡ä»¶

- [ ] ãƒ†ã‚¹ãƒˆãŒã™ã¹ã¦ãƒ‘ã‚¹ã™ã‚‹
- [ ] åŒã˜ç”»åƒã®å†è§£ææ™‚ã€APIå‘¼ã³å‡ºã—ãŒç™ºç”Ÿã—ãªã„
- [ ] ç•°ãªã‚‹è§£æã‚¿ã‚¤ãƒ—ï¼ˆtable/graphï¼‰ã§åˆ¥ã€…ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹
- [ ] PDFå‡¦ç†æ™‚é–“ãŒ30-50%çŸ­ç¸®ã•ã‚Œã‚‹ï¼ˆå†å‡¦ç†æ™‚ï¼‰

---

## ğŸ¯ ã‚¿ã‚¹ã‚¯ 5: BM25ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢

### æº–å‚™

- [ ] ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
  ```bash
  pip install rank-bm25==0.2.2
  pip install mecab-python3==1.0.6
  ```

- [ ] MeCabã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèªï¼ˆWindowsï¼‰
  ```bash
  python -c "import MeCab; print(MeCab.Tagger('-Owakati').parse('ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™'))"
  ```
  - ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹å ´åˆ: https://github.com/ikegami-yukino/mecab/releases ã‹ã‚‰ãƒã‚¤ãƒŠãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### å®Ÿè£…

- [ ] æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: `backend/src/hybrid_search.py`
  - [ ] `HybridSearcher`ã‚¯ãƒ©ã‚¹ã‚’å®Ÿè£…
  - [ ] `build_bm25_index()`: BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
  - [ ] `hybrid_search()`: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯
  - [ ] `_normalize_scores()`: ã‚¹ã‚³ã‚¢æ­£è¦åŒ–

<details>
<summary>ğŸ“ å®Ÿè£…ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ</summary>

```python
# backend/src/hybrid_search.py

from rank_bm25 import BM25Okapi
from typing import List, Dict, Tuple, Optional
import numpy as np
import MeCab
import logging

logger = logging.getLogger(__name__)

class HybridSearcher:
    def __init__(self, vector_store, alpha: float = 0.7):
        """
        ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®åˆæœŸåŒ–

        Args:
            vector_store: ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            alpha: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®é‡ã¿ï¼ˆ0-1ï¼‰
        """
        self.vector_store = vector_store
        self.alpha = alpha
        self.bm25_index = None
        self.bm25_docs = []
        self.mecab = MeCab.Tagger("-Owakati")
        logger.info(f"HybridSearcher initialized with alpha={alpha}")

    def build_bm25_index(self, documents: List[str]):
        """BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰"""
        logger.info(f"Building BM25 index for {len(documents)} documents...")

        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        tokenized_docs = [
            self.mecab.parse(doc).strip().split()
            for doc in documents
        ]

        self.bm25_docs = documents
        self.bm25_index = BM25Okapi(tokenized_docs)
        logger.info("BM25 index built successfully")

    def hybrid_search(
        self,
        query: str,
        query_embedding: List[float],
        k: int = 5,
        filter: Optional[Dict] = None
    ) -> List[Tuple[str, float]]:
        """
        ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆãƒ™ã‚¯ãƒˆãƒ« + BM25ï¼‰

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            query_embedding: ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿
            k: è¿”å´ã™ã‚‹ä»¶æ•°
            filter: ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ç­‰

        Returns:
            (ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ, ã‚¹ã‚³ã‚¢)ã®ãƒªã‚¹ãƒˆ
        """
        if not self.bm25_index:
            logger.warning("BM25 index not built, using vector search only")
            results = self.vector_store.similarity_search_with_score(
                query_embedding, k=k, filter=filter
            )
            return [(doc.page_content, score) for doc, score in results]

        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
        vector_results = self.vector_store.similarity_search_with_score(
            query_embedding,
            k=k*2,  # å¤šã‚ã«å–å¾—ã—ã¦èåˆ
            filter=filter
        )

        # BM25æ¤œç´¢
        tokenized_query = self.mecab.parse(query).strip().split()
        bm25_scores = self.bm25_index.get_scores(tokenized_query)

        # ã‚¹ã‚³ã‚¢æ­£è¦åŒ–
        vector_scores_norm = self._normalize_scores(
            [score for _, score in vector_results]
        )
        bm25_scores_norm = self._normalize_scores(bm25_scores)

        # èåˆã‚¹ã‚³ã‚¢è¨ˆç®—
        combined_scores = {}

        for idx, (doc, _) in enumerate(vector_results):
            doc_id = doc.metadata.get('id', idx)
            doc_content = doc.page_content
            combined_scores[doc_content] = self.alpha * vector_scores_norm[idx]

        for idx, bm25_score in enumerate(bm25_scores_norm):
            doc_content = self.bm25_docs[idx]
            if doc_content in combined_scores:
                combined_scores[doc_content] += (1 - self.alpha) * bm25_score
            else:
                combined_scores[doc_content] = (1 - self.alpha) * bm25_score

        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_results = sorted(
            combined_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:k]

        logger.debug(f"Hybrid search completed: {len(sorted_results)} results")
        return sorted_results

    def _normalize_scores(self, scores: List[float]) -> List[float]:
        """Min-Maxæ­£è¦åŒ–"""
        scores = np.array(scores)
        min_score = scores.min()
        max_score = scores.max()

        if max_score - min_score == 0:
            return [1.0] * len(scores)

        return ((scores - min_score) / (max_score - min_score)).tolist()
```
</details>

- [ ] `backend/src/rag_engine.py` ã‚’ä¿®æ­£
  - [ ] `HybridSearcher`ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
  - [ ] `__init__()`: HybridSearcherã‚’åˆæœŸåŒ–ã€BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰
  - [ ] `_get_relevant_contexts()`: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’ä½¿ç”¨

<details>
<summary>ğŸ“ çµ±åˆã‚³ãƒ¼ãƒ‰ä¾‹</summary>

```python
# backend/src/rag_engine.py ã®ä¿®æ­£ç®‡æ‰€

from src.hybrid_search import HybridSearcher

class RAGEngine:
    def __init__(self, config):
        # ...æ—¢å­˜ã‚³ãƒ¼ãƒ‰...

        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢åˆæœŸåŒ–
        if config["hybrid_search"].get("enabled", True):
            alpha = config["hybrid_search"].get("alpha", 0.7)
            self.hybrid_searcher = HybridSearcher(self.vector_store, alpha=alpha)

            # BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
            self._build_bm25_index()
        else:
            self.hybrid_searcher = None

    def _build_bm25_index(self):
        """å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰"""
        logger.info("Building BM25 index from all documents...")

        # Supabaseã‹ã‚‰å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
        all_docs = self.vector_store.get_all_documents()
        documents = [doc.page_content for doc in all_docs]

        self.hybrid_searcher.build_bm25_index(documents)

    def _get_relevant_contexts(
        self,
        query: str,
        category: Optional[str] = None
    ) -> List[Dict]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’ä½¿ç”¨"""
        query_embedding = self.embedder.embed_text(query)

        if self.hybrid_searcher:
            results = self.hybrid_searcher.hybrid_search(
                query,
                query_embedding,
                k=5,
                filter={"category": category} if category else None
            )
            # (doc, score)ã®ã‚¿ãƒ—ãƒ«ã‚’Dictã«å¤‰æ›
            return [{"content": doc, "score": score} for doc, score in results]
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿
            results = self.vector_store.similarity_search(
                query_embedding,
                k=5,
                filter={"category": category} if category else None
            )
            return [{"content": r.page_content} for r in results]
```
</details>

- [ ] `config.yaml` ã«è¨­å®šã‚’è¿½åŠ 
  ```yaml
  hybrid_search:
    enabled: true
    alpha: 0.7  # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®é‡ã¿
    bm25_k1: 1.5
    bm25_b: 0.75
  ```

### ãƒ†ã‚¹ãƒˆ

- [ ] ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’ä½œæˆ: `backend/tests/test_hybrid_search.py`
  - [ ] `test_bm25_index_building`: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ­£ã—ãæ§‹ç¯‰ã•ã‚Œã‚‹ã‹
  - [ ] `test_hybrid_search_keyword_query`: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¯ã‚¨ãƒªã§é©åˆ‡ãªçµæœãŒè¿”ã‚‹ã‹
  - [ ] `test_hybrid_search_semantic_query`: ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¯ã‚¨ãƒªã§é©åˆ‡ãªçµæœãŒè¿”ã‚‹ã‹

- [ ] çµ±åˆãƒ†ã‚¹ãƒˆ
  ```bash
  pytest tests/test_hybrid_search.py -v
  ```

- [ ] æ‰‹å‹•ãƒ†ã‚¹ãƒˆ: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒã¨æ„å‘³çš„ãƒãƒƒãƒã®ä¸¡æ–¹ã§æ¤œç´¢ç²¾åº¦ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’ç¢ºèª

### å®Œäº†æ¡ä»¶

- [ ] ãƒ†ã‚¹ãƒˆãŒã™ã¹ã¦ãƒ‘ã‚¹ã™ã‚‹
- [ ] BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ­£ã—ãæ§‹ç¯‰ã•ã‚Œã‚‹
- [ ] ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¯ã‚¨ãƒªã§BM25ã‚¹ã‚³ã‚¢ãŒåŠ¹ã
- [ ] ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚¯ã‚¨ãƒªã§ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ã‚³ã‚¢ãŒåŠ¹ã
- [ ] ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã§ä¸¡æ–¹ã®ãƒ¡ãƒªãƒƒãƒˆãŒæ´»ã‹ã•ã‚Œã‚‹

---

## ğŸ¯ ã‚¿ã‚¹ã‚¯ 6: çµ±åˆãƒ†ã‚¹ãƒˆ & ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

### çµ±åˆãƒ†ã‚¹ãƒˆ

- [ ] ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
  ```bash
  cd backend
  pytest tests/ -v --cov=src
  ```

- [ ] å…¨æ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã—ã¦PDFå‡¦ç†ãƒ†ã‚¹ãƒˆ
  - [ ] ã‚µãƒ³ãƒ—ãƒ«PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
  - [ ] PDFå‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã™ã‚‹
  - [ ] è³ªå•å¿œç­”ãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹
  - [ ] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒé©åˆ‡ã«å‹•ä½œã™ã‚‹

- [ ] æ©Ÿèƒ½ãƒ•ãƒ©ã‚°ã®ãƒ†ã‚¹ãƒˆ
  - [ ] å„æ©Ÿèƒ½ã‚’å€‹åˆ¥ã«æœ‰åŠ¹/ç„¡åŠ¹ã«ã—ã¦å‹•ä½œç¢ºèª
  - [ ] ã™ã¹ã¦ç„¡åŠ¹ã«ã—ã¦ã‚‚æ—¢å­˜æ©Ÿèƒ½ãŒå‹•ä½œã™ã‚‹

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

- [ ] ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¯ã‚¨ãƒªã‚»ãƒƒãƒˆã‚’æº–å‚™
  ```json
  // backend/evaluation/benchmark_queries.json
  [
    {"query": "è¡¨ã®ä½œæˆæ–¹æ³•", "category": "manual"},
    {"query": "ã‚°ãƒ©ãƒ•ã®ãƒ‡ãƒ¼ã‚¿ç¯„å›²å¤‰æ›´", "category": "manual"},
    // ...20-50ä»¶
  ]
  ```

- [ ] ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
  ```bash
  cd backend/evaluation
  python benchmark.py
  ```

- [ ] ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ¸¬å®š
  - [ ] æ¤œç´¢ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆP50, P95, P99ï¼‰
  - [ ] æ¤œç´¢ç²¾åº¦ï¼ˆNDCG@5, MRRï¼‰
  - [ ] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡
  - [ ] APIã‚³ã‚¹ãƒˆå‰Šæ¸›ç‡

- [ ] æ”¹å–„å‰å¾Œã®æ¯”è¼ƒè¡¨ã‚’ä½œæˆ

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ã®ç¢ºèª

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | æ”¹å–„å‰ | ç›®æ¨™å€¤ | å®Ÿæ¸¬å€¤ | é”æˆ |
|-----------|--------|--------|--------|------|
| æ¤œç´¢ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆP50ï¼‰ | 200-500ms | 100-300ms | ___ms | [ ] |
| æ¤œç´¢ç²¾åº¦ï¼ˆNDCG@5ï¼‰ | 0.85 | 0.92 | ___ | [ ] |
| ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ | 0% | 60-70% | ___% | [ ] |
| APIã‚³ã‚¹ãƒˆå‰Šæ¸› | - | 30-40% | ___% | [ ] |

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°

- [ ] README.md ã‚’æ›´æ–°
  - [ ] æ–°æ©Ÿèƒ½ã®èª¬æ˜ã‚’è¿½åŠ 
  - [ ] è¨­å®šä¾‹ã‚’è¿½åŠ 

- [ ] config.yaml ã«ã‚³ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ 
  - [ ] å„è¨­å®šé …ç›®ã®èª¬æ˜
  - [ ] æ¨å¥¨å€¤

- [ ] CHANGELOG.md ã‚’ä½œæˆ/æ›´æ–°
  ```markdown
  # Changelog

  ## [Phase 1] - 2025-XX-XX

  ### Added
  - Rerankingå°å…¥ï¼ˆæ¤œç´¢ç²¾åº¦+10-15%ï¼‰
  - ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°
  - ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
  - Visionè§£æã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
  - BM25ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢

  ### Performance
  - æ¤œç´¢ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: 200-500ms â†’ 100-300ms
  - æ¤œç´¢ç²¾åº¦ï¼ˆNDCG@5ï¼‰: 0.85 â†’ 0.92
  - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: 60-70%
  ```

### å®Œäº†æ¡ä»¶

- [ ] ã™ã¹ã¦ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆãŒãƒ‘ã‚¹ã™ã‚‹
- [ ] çµ±åˆãƒ†ã‚¹ãƒˆãŒãƒ‘ã‚¹ã™ã‚‹
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ã‚’é”æˆã—ã¦ã„ã‚‹
- [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒæ›´æ–°ã•ã‚Œã¦ã„ã‚‹
- [ ] ã‚³ãƒ¼ãƒ‰ãŒmasterãƒ–ãƒ©ãƒ³ãƒã«ãƒãƒ¼ã‚¸ã•ã‚Œã¦ã„ã‚‹

---

## ğŸ“Š æœ€çµ‚ãƒ¬ãƒ“ãƒ¥ãƒ¼

### ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼

- [ ] ã‚³ãƒ¼ãƒ‰ã‚¹ã‚¿ã‚¤ãƒ«ãŒçµ±ä¸€ã•ã‚Œã¦ã„ã‚‹ï¼ˆBlack, flake8ï¼‰
- [ ] å‹ãƒ’ãƒ³ãƒˆãŒé©åˆ‡ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹
- [ ] DocstringãŒé©åˆ‡ã«è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹
- [ ] ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒé©åˆ‡
- [ ] ãƒ­ã‚°ãŒé©åˆ‡ã«è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£

- [ ] APIã‚­ãƒ¼ãŒãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„
- [ ] å…¥åŠ›æ¤œè¨¼ãŒé©åˆ‡
- [ ] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ãƒ¼ãƒŸãƒƒã‚·ãƒ§ãƒ³ãŒé©åˆ‡

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

- [ ] ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãŒãªã„
- [ ] ä¸è¦ãªå†è¨ˆç®—ãŒãªã„
- [ ] ä¸¦åˆ—å‡¦ç†ãŒé©åˆ‡ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹

### ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“

- [ ] ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒåˆ†ã‹ã‚Šã‚„ã™ã„
- [ ] ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒé«˜é€Ÿ
- [ ] å›ç­”ã®è³ªãŒå‘ä¸Šã—ã¦ã„ã‚‹

---

## ğŸ‰ Phase 1 å®Œäº†ï¼

ã™ã¹ã¦ã®ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ãŸã‚‰:

1. [ ] æˆæœã‚’ãƒãƒ¼ãƒ ã«å…±æœ‰
2. [ ] ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åé›†
3. [ ] Phase 2ã®è¨ˆç”»ã‚’é–‹å§‹

**ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼** ğŸš€

---

**ä½œæˆæ—¥**: 2025-11-04
**æœ€çµ‚æ›´æ–°**: ___________
**æ‹…å½“è€…**: ___________
